{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">#04. Why Neural Networks Deeply Learn a Mathematical Formula?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book + Private Lessons [Here ‚Üó](https://sotastica.com/reservar)\n",
    "- Subscribe to my [Blog ‚Üó](https://blog.pythonassembly.com/)\n",
    "- Let's keep in touch on [LinkedIn ‚Üó](www.linkedin.com/in/jsulopz) üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The Machine Learns...\n",
    ">\n",
    "> But, **what does it learn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Machine Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Practical Example ‚Üí [Tesla Autopilot](https://www.tesla.com/AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example where It Fails ‚Üí [Tesla Confuses Moon with Semaphore](https://twitter.com/Carnage4Life/status/1418920100086784000?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simply execute the following lines of code to load the data.\n",
    "> - This dataset contains **statistics about Car Accidents** (columns)\n",
    "> - In each one of **USA States** (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fivethirtyeight/fivethirtyeight-bad-drivers-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LA</th>\n",
       "      <td>20.5</td>\n",
       "      <td>7.175</td>\n",
       "      <td>6.765</td>\n",
       "      <td>14.965</td>\n",
       "      <td>20.090</td>\n",
       "      <td>1281.55</td>\n",
       "      <td>194.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA</th>\n",
       "      <td>18.2</td>\n",
       "      <td>9.100</td>\n",
       "      <td>5.642</td>\n",
       "      <td>17.472</td>\n",
       "      <td>16.016</td>\n",
       "      <td>905.99</td>\n",
       "      <td>153.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SC</th>\n",
       "      <td>23.9</td>\n",
       "      <td>9.082</td>\n",
       "      <td>9.799</td>\n",
       "      <td>22.944</td>\n",
       "      <td>19.359</td>\n",
       "      <td>858.97</td>\n",
       "      <td>116.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WA</th>\n",
       "      <td>10.6</td>\n",
       "      <td>4.452</td>\n",
       "      <td>3.498</td>\n",
       "      <td>8.692</td>\n",
       "      <td>9.116</td>\n",
       "      <td>890.03</td>\n",
       "      <td>111.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KY</th>\n",
       "      <td>21.4</td>\n",
       "      <td>4.066</td>\n",
       "      <td>4.922</td>\n",
       "      <td>16.692</td>\n",
       "      <td>16.264</td>\n",
       "      <td>872.51</td>\n",
       "      <td>137.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM</th>\n",
       "      <td>18.4</td>\n",
       "      <td>3.496</td>\n",
       "      <td>4.968</td>\n",
       "      <td>12.328</td>\n",
       "      <td>18.032</td>\n",
       "      <td>869.85</td>\n",
       "      <td>120.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT</th>\n",
       "      <td>21.4</td>\n",
       "      <td>8.346</td>\n",
       "      <td>9.416</td>\n",
       "      <td>17.976</td>\n",
       "      <td>18.190</td>\n",
       "      <td>816.21</td>\n",
       "      <td>85.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>12.5</td>\n",
       "      <td>4.250</td>\n",
       "      <td>4.000</td>\n",
       "      <td>8.875</td>\n",
       "      <td>12.375</td>\n",
       "      <td>1048.78</td>\n",
       "      <td>192.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>23.9</td>\n",
       "      <td>5.497</td>\n",
       "      <td>10.038</td>\n",
       "      <td>23.661</td>\n",
       "      <td>20.554</td>\n",
       "      <td>688.75</td>\n",
       "      <td>109.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>16.1</td>\n",
       "      <td>6.923</td>\n",
       "      <td>5.474</td>\n",
       "      <td>14.812</td>\n",
       "      <td>13.524</td>\n",
       "      <td>790.32</td>\n",
       "      <td>144.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "LA       20.5     7.175    6.765          14.965       20.090      1281.55   \n",
       "PA       18.2     9.100    5.642          17.472       16.016       905.99   \n",
       "SC       23.9     9.082    9.799          22.944       19.359       858.97   \n",
       "WA       10.6     4.452    3.498           8.692        9.116       890.03   \n",
       "KY       21.4     4.066    4.922          16.692       16.264       872.51   \n",
       "NM       18.4     3.496    4.968          12.328       18.032       869.85   \n",
       "MT       21.4     8.346    9.416          17.976       18.190       816.21   \n",
       "MD       12.5     4.250    4.000           8.875       12.375      1048.78   \n",
       "ND       23.9     5.497   10.038          23.661       20.554       688.75   \n",
       "MO       16.1     6.923    5.474          14.812       13.524       790.32   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "LA          194.78  \n",
       "PA          153.86  \n",
       "SC          116.29  \n",
       "WA          111.62  \n",
       "KY          137.13  \n",
       "NM          120.75  \n",
       "MT           85.15  \n",
       "MD          192.70  \n",
       "ND          109.72  \n",
       "MO          144.45  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(name='car_crashes', index_col='abbrev')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 31/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 32/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 33/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 34/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 35/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 36/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 37/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 38/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 39/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 40/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 41/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 42/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 43/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 44/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 45/150\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 46/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 47/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 48/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 49/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 50/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 51/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 52/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 53/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 54/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 55/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 56/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 57/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 58/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 59/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 60/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 61/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 62/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 63/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 64/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 65/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 66/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 67/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 68/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 69/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 70/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 71/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 72/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 73/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 74/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 75/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 76/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 77/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 78/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 79/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 80/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 81/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 82/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 83/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 84/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 85/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 86/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 87/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 88/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 89/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 90/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 91/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 92/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 93/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 94/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 95/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 96/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 97/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 98/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 99/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 100/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 101/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 102/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 103/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 104/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 105/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 106/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 107/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 108/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 109/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 110/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 111/150\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 112/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 113/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 114/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 115/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 116/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 117/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 118/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 119/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 120/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 121/150\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 122/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 123/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 124/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 125/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 126/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 127/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 128/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 129/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 130/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 131/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 132/150\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 133/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 134/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 135/150\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 136/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 137/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 138/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 139/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 140/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 141/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 142/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 143/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 144/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 145/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 146/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9881\n",
      "Epoch 147/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 148/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9881 - mse: 265.9880\n",
      "Epoch 149/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n",
      "Epoch 150/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 265.9880 - mse: 265.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c1a2550>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/1110569478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# Legacy graph support is contained in `training_v1.Model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2788\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2790\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2791\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the `Weights`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/3324602025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/22777151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "algo.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`algo = ?` | `algo.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3316416695.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/3316416695.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    algo =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "algo = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.layers.core.Dense"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(3))\n",
    "model.add(layer=Dense(1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.layers.core.Dense"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Dense(units=3, input_dim=6, kernel_initializer='zeros'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:06:18.109493: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "AL       18.8     7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.7401712],\n",
       "        [-0.8742442],\n",
       "        [-0.2580135]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 - 0s - loss: 207.5527 - mse: 207.5527\n",
      "Epoch 2/500\n",
      "2/2 - 0s - loss: 51.2457 - mse: 51.2457\n",
      "Epoch 3/500\n",
      "2/2 - 0s - loss: 30.1981 - mse: 30.1981\n",
      "Epoch 4/500\n",
      "2/2 - 0s - loss: 27.8662 - mse: 27.8662\n",
      "Epoch 5/500\n",
      "2/2 - 0s - loss: 27.3868 - mse: 27.3868\n",
      "Epoch 6/500\n",
      "2/2 - 0s - loss: 26.9816 - mse: 26.9816\n",
      "Epoch 7/500\n",
      "2/2 - 0s - loss: 27.0165 - mse: 27.0165\n",
      "Epoch 8/500\n",
      "2/2 - 0s - loss: 28.1204 - mse: 28.1204\n",
      "Epoch 9/500\n",
      "2/2 - 0s - loss: 27.5292 - mse: 27.5292\n",
      "Epoch 10/500\n",
      "2/2 - 0s - loss: 26.0097 - mse: 26.0097\n",
      "Epoch 11/500\n",
      "2/2 - 0s - loss: 25.6759 - mse: 25.6759\n",
      "Epoch 12/500\n",
      "2/2 - 0s - loss: 25.4211 - mse: 25.4211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:06:20.842633: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "2/2 - 0s - loss: 25.5946 - mse: 25.5946\n",
      "Epoch 14/500\n",
      "2/2 - 0s - loss: 25.3454 - mse: 25.3454\n",
      "Epoch 15/500\n",
      "2/2 - 0s - loss: 24.7865 - mse: 24.7865\n",
      "Epoch 16/500\n",
      "2/2 - 0s - loss: 25.7840 - mse: 25.7840\n",
      "Epoch 17/500\n",
      "2/2 - 0s - loss: 24.6461 - mse: 24.6461\n",
      "Epoch 18/500\n",
      "2/2 - 0s - loss: 23.7738 - mse: 23.7738\n",
      "Epoch 19/500\n",
      "2/2 - 0s - loss: 22.9091 - mse: 22.9091\n",
      "Epoch 20/500\n",
      "2/2 - 0s - loss: 22.5686 - mse: 22.5686\n",
      "Epoch 21/500\n",
      "2/2 - 0s - loss: 22.2095 - mse: 22.2095\n",
      "Epoch 22/500\n",
      "2/2 - 0s - loss: 25.0059 - mse: 25.0059\n",
      "Epoch 23/500\n",
      "2/2 - 0s - loss: 24.7641 - mse: 24.7641\n",
      "Epoch 24/500\n",
      "2/2 - 0s - loss: 22.0349 - mse: 22.0349\n",
      "Epoch 25/500\n",
      "2/2 - 0s - loss: 23.9454 - mse: 23.9454\n",
      "Epoch 26/500\n",
      "2/2 - 0s - loss: 20.2646 - mse: 20.2646\n",
      "Epoch 27/500\n",
      "2/2 - 0s - loss: 20.1119 - mse: 20.1119\n",
      "Epoch 28/500\n",
      "2/2 - 0s - loss: 21.6248 - mse: 21.6248\n",
      "Epoch 29/500\n",
      "2/2 - 0s - loss: 20.9176 - mse: 20.9176\n",
      "Epoch 30/500\n",
      "2/2 - 0s - loss: 23.2677 - mse: 23.2677\n",
      "Epoch 31/500\n",
      "2/2 - 0s - loss: 20.7520 - mse: 20.7520\n",
      "Epoch 32/500\n",
      "2/2 - 0s - loss: 20.6201 - mse: 20.6201\n",
      "Epoch 33/500\n",
      "2/2 - 0s - loss: 20.2240 - mse: 20.2240\n",
      "Epoch 34/500\n",
      "2/2 - 0s - loss: 22.4100 - mse: 22.4100\n",
      "Epoch 35/500\n",
      "2/2 - 0s - loss: 22.9241 - mse: 22.9241\n",
      "Epoch 36/500\n",
      "2/2 - 0s - loss: 19.8446 - mse: 19.8446\n",
      "Epoch 37/500\n",
      "2/2 - 0s - loss: 20.4272 - mse: 20.4272\n",
      "Epoch 38/500\n",
      "2/2 - 0s - loss: 21.2085 - mse: 21.2085\n",
      "Epoch 39/500\n",
      "2/2 - 0s - loss: 18.3537 - mse: 18.3537\n",
      "Epoch 40/500\n",
      "2/2 - 0s - loss: 16.4834 - mse: 16.4834\n",
      "Epoch 41/500\n",
      "2/2 - 0s - loss: 16.0971 - mse: 16.0971\n",
      "Epoch 42/500\n",
      "2/2 - 0s - loss: 17.3830 - mse: 17.3830\n",
      "Epoch 43/500\n",
      "2/2 - 0s - loss: 15.6102 - mse: 15.6102\n",
      "Epoch 44/500\n",
      "2/2 - 0s - loss: 15.6013 - mse: 15.6013\n",
      "Epoch 45/500\n",
      "2/2 - 0s - loss: 14.8663 - mse: 14.8663\n",
      "Epoch 46/500\n",
      "2/2 - 0s - loss: 14.5439 - mse: 14.5439\n",
      "Epoch 47/500\n",
      "2/2 - 0s - loss: 15.6801 - mse: 15.6801\n",
      "Epoch 48/500\n",
      "2/2 - 0s - loss: 20.3148 - mse: 20.3148\n",
      "Epoch 49/500\n",
      "2/2 - 0s - loss: 14.7773 - mse: 14.7773\n",
      "Epoch 50/500\n",
      "2/2 - 0s - loss: 15.9743 - mse: 15.9743\n",
      "Epoch 51/500\n",
      "2/2 - 0s - loss: 15.7789 - mse: 15.7789\n",
      "Epoch 52/500\n",
      "2/2 - 0s - loss: 13.0821 - mse: 13.0821\n",
      "Epoch 53/500\n",
      "2/2 - 0s - loss: 15.1518 - mse: 15.1518\n",
      "Epoch 54/500\n",
      "2/2 - 0s - loss: 18.6381 - mse: 18.6381\n",
      "Epoch 55/500\n",
      "2/2 - 0s - loss: 12.8744 - mse: 12.8744\n",
      "Epoch 56/500\n",
      "2/2 - 0s - loss: 13.6409 - mse: 13.6409\n",
      "Epoch 57/500\n",
      "2/2 - 0s - loss: 12.0817 - mse: 12.0817\n",
      "Epoch 58/500\n",
      "2/2 - 0s - loss: 12.0078 - mse: 12.0078\n",
      "Epoch 59/500\n",
      "2/2 - 0s - loss: 11.9149 - mse: 11.9149\n",
      "Epoch 60/500\n",
      "2/2 - 0s - loss: 11.7211 - mse: 11.7211\n",
      "Epoch 61/500\n",
      "2/2 - 0s - loss: 11.4703 - mse: 11.4703\n",
      "Epoch 62/500\n",
      "2/2 - 0s - loss: 10.8989 - mse: 10.8989\n",
      "Epoch 63/500\n",
      "2/2 - 0s - loss: 12.7186 - mse: 12.7186\n",
      "Epoch 64/500\n",
      "2/2 - 0s - loss: 11.7376 - mse: 11.7376\n",
      "Epoch 65/500\n",
      "2/2 - 0s - loss: 12.4935 - mse: 12.4935\n",
      "Epoch 66/500\n",
      "2/2 - 0s - loss: 10.1590 - mse: 10.1590\n",
      "Epoch 67/500\n",
      "2/2 - 0s - loss: 11.7578 - mse: 11.7578\n",
      "Epoch 68/500\n",
      "2/2 - 0s - loss: 10.2200 - mse: 10.2200\n",
      "Epoch 69/500\n",
      "2/2 - 0s - loss: 15.1594 - mse: 15.1594\n",
      "Epoch 70/500\n",
      "2/2 - 0s - loss: 9.5875 - mse: 9.5875\n",
      "Epoch 71/500\n",
      "2/2 - 0s - loss: 8.9896 - mse: 8.9896\n",
      "Epoch 72/500\n",
      "2/2 - 0s - loss: 9.0564 - mse: 9.0564\n",
      "Epoch 73/500\n",
      "2/2 - 0s - loss: 8.5401 - mse: 8.5401\n",
      "Epoch 74/500\n",
      "2/2 - 0s - loss: 9.1544 - mse: 9.1544\n",
      "Epoch 75/500\n",
      "2/2 - 0s - loss: 11.6217 - mse: 11.6217\n",
      "Epoch 76/500\n",
      "2/2 - 0s - loss: 8.5867 - mse: 8.5867\n",
      "Epoch 77/500\n",
      "2/2 - 0s - loss: 7.8339 - mse: 7.8339\n",
      "Epoch 78/500\n",
      "2/2 - 0s - loss: 8.8456 - mse: 8.8456\n",
      "Epoch 79/500\n",
      "2/2 - 0s - loss: 8.7276 - mse: 8.7276\n",
      "Epoch 80/500\n",
      "2/2 - 0s - loss: 7.4109 - mse: 7.4109\n",
      "Epoch 81/500\n",
      "2/2 - 0s - loss: 7.1330 - mse: 7.1330\n",
      "Epoch 82/500\n",
      "2/2 - 0s - loss: 10.0629 - mse: 10.0629\n",
      "Epoch 83/500\n",
      "2/2 - 0s - loss: 10.2820 - mse: 10.2820\n",
      "Epoch 84/500\n",
      "2/2 - 0s - loss: 6.7727 - mse: 6.7727\n",
      "Epoch 85/500\n",
      "2/2 - 0s - loss: 6.6327 - mse: 6.6327\n",
      "Epoch 86/500\n",
      "2/2 - 0s - loss: 6.7910 - mse: 6.7910\n",
      "Epoch 87/500\n",
      "2/2 - 0s - loss: 6.3416 - mse: 6.3416\n",
      "Epoch 88/500\n",
      "2/2 - 0s - loss: 6.6899 - mse: 6.6899\n",
      "Epoch 89/500\n",
      "2/2 - 0s - loss: 8.2980 - mse: 8.2980\n",
      "Epoch 90/500\n",
      "2/2 - 0s - loss: 8.4299 - mse: 8.4299\n",
      "Epoch 91/500\n",
      "2/2 - 0s - loss: 8.9047 - mse: 8.9047\n",
      "Epoch 92/500\n",
      "2/2 - 0s - loss: 6.8112 - mse: 6.8112\n",
      "Epoch 93/500\n",
      "2/2 - 0s - loss: 5.6413 - mse: 5.6413\n",
      "Epoch 94/500\n",
      "2/2 - 0s - loss: 5.6601 - mse: 5.6601\n",
      "Epoch 95/500\n",
      "2/2 - 0s - loss: 7.4094 - mse: 7.4094\n",
      "Epoch 96/500\n",
      "2/2 - 0s - loss: 11.5475 - mse: 11.5475\n",
      "Epoch 97/500\n",
      "2/2 - 0s - loss: 6.6260 - mse: 6.6260\n",
      "Epoch 98/500\n",
      "2/2 - 0s - loss: 5.1054 - mse: 5.1054\n",
      "Epoch 99/500\n",
      "2/2 - 0s - loss: 5.1507 - mse: 5.1507\n",
      "Epoch 100/500\n",
      "2/2 - 0s - loss: 6.4913 - mse: 6.4913\n",
      "Epoch 101/500\n",
      "2/2 - 0s - loss: 7.2180 - mse: 7.2180\n",
      "Epoch 102/500\n",
      "2/2 - 0s - loss: 5.2397 - mse: 5.2397\n",
      "Epoch 103/500\n",
      "2/2 - 0s - loss: 5.3244 - mse: 5.3244\n",
      "Epoch 104/500\n",
      "2/2 - 0s - loss: 4.8091 - mse: 4.8091\n",
      "Epoch 105/500\n",
      "2/2 - 0s - loss: 5.6798 - mse: 5.6798\n",
      "Epoch 106/500\n",
      "2/2 - 0s - loss: 4.4128 - mse: 4.4128\n",
      "Epoch 107/500\n",
      "2/2 - 0s - loss: 4.4141 - mse: 4.4141\n",
      "Epoch 108/500\n",
      "2/2 - 0s - loss: 4.9829 - mse: 4.9829\n",
      "Epoch 109/500\n",
      "2/2 - 0s - loss: 4.2968 - mse: 4.2968\n",
      "Epoch 110/500\n",
      "2/2 - 0s - loss: 3.9884 - mse: 3.9884\n",
      "Epoch 111/500\n",
      "2/2 - 0s - loss: 3.9924 - mse: 3.9924\n",
      "Epoch 112/500\n",
      "2/2 - 0s - loss: 6.3633 - mse: 6.3633\n",
      "Epoch 113/500\n",
      "2/2 - 0s - loss: 7.8778 - mse: 7.8778\n",
      "Epoch 114/500\n",
      "2/2 - 0s - loss: 5.8546 - mse: 5.8546\n",
      "Epoch 115/500\n",
      "2/2 - 0s - loss: 3.8787 - mse: 3.8787\n",
      "Epoch 116/500\n",
      "2/2 - 0s - loss: 3.9285 - mse: 3.9285\n",
      "Epoch 117/500\n",
      "2/2 - 0s - loss: 3.7979 - mse: 3.7979\n",
      "Epoch 118/500\n",
      "2/2 - 0s - loss: 3.6134 - mse: 3.6134\n",
      "Epoch 119/500\n",
      "2/2 - 0s - loss: 3.9303 - mse: 3.9303\n",
      "Epoch 120/500\n",
      "2/2 - 0s - loss: 9.2272 - mse: 9.2272\n",
      "Epoch 121/500\n",
      "2/2 - 0s - loss: 6.8199 - mse: 6.8199\n",
      "Epoch 122/500\n",
      "2/2 - 0s - loss: 3.7262 - mse: 3.7262\n",
      "Epoch 123/500\n",
      "2/2 - 0s - loss: 3.2424 - mse: 3.2424\n",
      "Epoch 124/500\n",
      "2/2 - 0s - loss: 3.1423 - mse: 3.1423\n",
      "Epoch 125/500\n",
      "2/2 - 0s - loss: 4.3164 - mse: 4.3164\n",
      "Epoch 126/500\n",
      "2/2 - 0s - loss: 4.7015 - mse: 4.7015\n",
      "Epoch 127/500\n",
      "2/2 - 0s - loss: 3.1441 - mse: 3.1441\n",
      "Epoch 128/500\n",
      "2/2 - 0s - loss: 3.2662 - mse: 3.2662\n",
      "Epoch 129/500\n",
      "2/2 - 0s - loss: 3.6329 - mse: 3.6329\n",
      "Epoch 130/500\n",
      "2/2 - 0s - loss: 7.2036 - mse: 7.2036\n",
      "Epoch 131/500\n",
      "2/2 - 0s - loss: 5.7207 - mse: 5.7207\n",
      "Epoch 132/500\n",
      "2/2 - 0s - loss: 2.8687 - mse: 2.8687\n",
      "Epoch 133/500\n",
      "2/2 - 0s - loss: 3.4665 - mse: 3.4665\n",
      "Epoch 134/500\n",
      "2/2 - 0s - loss: 3.6014 - mse: 3.6014\n",
      "Epoch 135/500\n",
      "2/2 - 0s - loss: 3.2651 - mse: 3.2651\n",
      "Epoch 136/500\n",
      "2/2 - 0s - loss: 2.7487 - mse: 2.7487\n",
      "Epoch 137/500\n",
      "2/2 - 0s - loss: 3.3510 - mse: 3.3510\n",
      "Epoch 138/500\n",
      "2/2 - 0s - loss: 4.4970 - mse: 4.4970\n",
      "Epoch 139/500\n",
      "2/2 - 0s - loss: 5.8232 - mse: 5.8232\n",
      "Epoch 140/500\n",
      "2/2 - 0s - loss: 2.7932 - mse: 2.7932\n",
      "Epoch 141/500\n",
      "2/2 - 0s - loss: 2.5277 - mse: 2.5277\n",
      "Epoch 142/500\n",
      "2/2 - 0s - loss: 2.5225 - mse: 2.5225\n",
      "Epoch 143/500\n",
      "2/2 - 0s - loss: 2.6855 - mse: 2.6855\n",
      "Epoch 144/500\n",
      "2/2 - 0s - loss: 2.6840 - mse: 2.6840\n",
      "Epoch 145/500\n",
      "2/2 - 0s - loss: 5.7394 - mse: 5.7394\n",
      "Epoch 146/500\n",
      "2/2 - 0s - loss: 6.9648 - mse: 6.9648\n",
      "Epoch 147/500\n",
      "2/2 - 0s - loss: 3.1214 - mse: 3.1214\n",
      "Epoch 148/500\n",
      "2/2 - 0s - loss: 2.8309 - mse: 2.8309\n",
      "Epoch 149/500\n",
      "2/2 - 0s - loss: 2.3908 - mse: 2.3908\n",
      "Epoch 150/500\n",
      "2/2 - 0s - loss: 2.8773 - mse: 2.8773\n",
      "Epoch 151/500\n",
      "2/2 - 0s - loss: 8.1937 - mse: 8.1937\n",
      "Epoch 152/500\n",
      "2/2 - 0s - loss: 3.7890 - mse: 3.7890\n",
      "Epoch 153/500\n",
      "2/2 - 0s - loss: 2.2983 - mse: 2.2983\n",
      "Epoch 154/500\n",
      "2/2 - 0s - loss: 3.9313 - mse: 3.9313\n",
      "Epoch 155/500\n",
      "2/2 - 0s - loss: 3.5985 - mse: 3.5985\n",
      "Epoch 156/500\n",
      "2/2 - 0s - loss: 2.3568 - mse: 2.3568\n",
      "Epoch 157/500\n",
      "2/2 - 0s - loss: 2.7920 - mse: 2.7920\n",
      "Epoch 158/500\n",
      "2/2 - 0s - loss: 2.7910 - mse: 2.7910\n",
      "Epoch 159/500\n",
      "2/2 - 0s - loss: 2.3346 - mse: 2.3346\n",
      "Epoch 160/500\n",
      "2/2 - 0s - loss: 2.7509 - mse: 2.7509\n",
      "Epoch 161/500\n",
      "2/2 - 0s - loss: 2.3953 - mse: 2.3953\n",
      "Epoch 162/500\n",
      "2/2 - 0s - loss: 3.1232 - mse: 3.1232\n",
      "Epoch 163/500\n",
      "2/2 - 0s - loss: 6.2919 - mse: 6.2919\n",
      "Epoch 164/500\n",
      "2/2 - 0s - loss: 3.3575 - mse: 3.3575\n",
      "Epoch 165/500\n",
      "2/2 - 0s - loss: 3.5125 - mse: 3.5125\n",
      "Epoch 166/500\n",
      "2/2 - 0s - loss: 4.1542 - mse: 4.1542\n",
      "Epoch 167/500\n",
      "2/2 - 0s - loss: 2.5186 - mse: 2.5186\n",
      "Epoch 168/500\n",
      "2/2 - 0s - loss: 2.0494 - mse: 2.0494\n",
      "Epoch 169/500\n",
      "2/2 - 0s - loss: 2.1308 - mse: 2.1308\n",
      "Epoch 170/500\n",
      "2/2 - 0s - loss: 4.7896 - mse: 4.7896\n",
      "Epoch 171/500\n",
      "2/2 - 0s - loss: 9.5356 - mse: 9.5356\n",
      "Epoch 172/500\n",
      "2/2 - 0s - loss: 3.9369 - mse: 3.9369\n",
      "Epoch 173/500\n",
      "2/2 - 0s - loss: 2.1701 - mse: 2.1701\n",
      "Epoch 174/500\n",
      "2/2 - 0s - loss: 2.1923 - mse: 2.1923\n",
      "Epoch 175/500\n",
      "2/2 - 0s - loss: 1.9773 - mse: 1.9773\n",
      "Epoch 176/500\n",
      "2/2 - 0s - loss: 1.9903 - mse: 1.9903\n",
      "Epoch 177/500\n",
      "2/2 - 0s - loss: 1.9192 - mse: 1.9192\n",
      "Epoch 178/500\n",
      "2/2 - 0s - loss: 1.9361 - mse: 1.9361\n",
      "Epoch 179/500\n",
      "2/2 - 0s - loss: 1.8659 - mse: 1.8659\n",
      "Epoch 180/500\n",
      "2/2 - 0s - loss: 2.6992 - mse: 2.6992\n",
      "Epoch 181/500\n",
      "2/2 - 0s - loss: 7.4049 - mse: 7.4049\n",
      "Epoch 182/500\n",
      "2/2 - 0s - loss: 4.9116 - mse: 4.9116\n",
      "Epoch 183/500\n",
      "2/2 - 0s - loss: 3.3215 - mse: 3.3215\n",
      "Epoch 184/500\n",
      "2/2 - 0s - loss: 2.7299 - mse: 2.7299\n",
      "Epoch 185/500\n",
      "2/2 - 0s - loss: 2.2470 - mse: 2.2470\n",
      "Epoch 186/500\n",
      "2/2 - 0s - loss: 2.5238 - mse: 2.5238\n",
      "Epoch 187/500\n",
      "2/2 - 0s - loss: 3.0798 - mse: 3.0798\n",
      "Epoch 188/500\n",
      "2/2 - 0s - loss: 3.5730 - mse: 3.5730\n",
      "Epoch 189/500\n",
      "2/2 - 0s - loss: 2.5738 - mse: 2.5738\n",
      "Epoch 190/500\n",
      "2/2 - 0s - loss: 2.2611 - mse: 2.2611\n",
      "Epoch 191/500\n",
      "2/2 - 0s - loss: 3.4346 - mse: 3.4346\n",
      "Epoch 192/500\n",
      "2/2 - 0s - loss: 5.9240 - mse: 5.9240\n",
      "Epoch 193/500\n",
      "2/2 - 0s - loss: 2.2597 - mse: 2.2597\n",
      "Epoch 194/500\n",
      "2/2 - 0s - loss: 1.8281 - mse: 1.8281\n",
      "Epoch 195/500\n",
      "2/2 - 0s - loss: 1.9617 - mse: 1.9617\n",
      "Epoch 196/500\n",
      "2/2 - 0s - loss: 5.8727 - mse: 5.8727\n",
      "Epoch 197/500\n",
      "2/2 - 0s - loss: 3.8874 - mse: 3.8874\n",
      "Epoch 198/500\n",
      "2/2 - 0s - loss: 1.9707 - mse: 1.9707\n",
      "Epoch 199/500\n",
      "2/2 - 0s - loss: 1.8779 - mse: 1.8779\n",
      "Epoch 200/500\n",
      "2/2 - 0s - loss: 3.8099 - mse: 3.8099\n",
      "Epoch 201/500\n",
      "2/2 - 0s - loss: 6.6686 - mse: 6.6686\n",
      "Epoch 202/500\n",
      "2/2 - 0s - loss: 4.8357 - mse: 4.8357\n",
      "Epoch 203/500\n",
      "2/2 - 0s - loss: 3.9801 - mse: 3.9801\n",
      "Epoch 204/500\n",
      "2/2 - 0s - loss: 2.2256 - mse: 2.2256\n",
      "Epoch 205/500\n",
      "2/2 - 0s - loss: 1.7307 - mse: 1.7307\n",
      "Epoch 206/500\n",
      "2/2 - 0s - loss: 1.7530 - mse: 1.7530\n",
      "Epoch 207/500\n",
      "2/2 - 0s - loss: 2.2400 - mse: 2.2400\n",
      "Epoch 208/500\n",
      "2/2 - 0s - loss: 2.6149 - mse: 2.6149\n",
      "Epoch 209/500\n",
      "2/2 - 0s - loss: 2.6086 - mse: 2.6086\n",
      "Epoch 210/500\n",
      "2/2 - 0s - loss: 5.5954 - mse: 5.5954\n",
      "Epoch 211/500\n",
      "2/2 - 0s - loss: 5.5417 - mse: 5.5417\n",
      "Epoch 212/500\n",
      "2/2 - 0s - loss: 2.6550 - mse: 2.6550\n",
      "Epoch 213/500\n",
      "2/2 - 0s - loss: 2.0104 - mse: 2.0104\n",
      "Epoch 214/500\n",
      "2/2 - 0s - loss: 1.7334 - mse: 1.7334\n",
      "Epoch 215/500\n",
      "2/2 - 0s - loss: 1.6966 - mse: 1.6966\n",
      "Epoch 216/500\n",
      "2/2 - 0s - loss: 1.7155 - mse: 1.7155\n",
      "Epoch 217/500\n",
      "2/2 - 0s - loss: 2.2522 - mse: 2.2522\n",
      "Epoch 218/500\n",
      "2/2 - 0s - loss: 3.1189 - mse: 3.1189\n",
      "Epoch 219/500\n",
      "2/2 - 0s - loss: 5.7282 - mse: 5.7282\n",
      "Epoch 220/500\n",
      "2/2 - 0s - loss: 4.3751 - mse: 4.3751\n",
      "Epoch 221/500\n",
      "2/2 - 0s - loss: 2.5528 - mse: 2.5528\n",
      "Epoch 222/500\n",
      "2/2 - 0s - loss: 2.4712 - mse: 2.4712\n",
      "Epoch 223/500\n",
      "2/2 - 0s - loss: 2.2694 - mse: 2.2694\n",
      "Epoch 224/500\n",
      "2/2 - 0s - loss: 2.2353 - mse: 2.2353\n",
      "Epoch 225/500\n",
      "2/2 - 0s - loss: 3.4972 - mse: 3.4972\n",
      "Epoch 226/500\n",
      "2/2 - 0s - loss: 3.0229 - mse: 3.0229\n",
      "Epoch 227/500\n",
      "2/2 - 0s - loss: 2.3494 - mse: 2.3494\n",
      "Epoch 228/500\n",
      "2/2 - 0s - loss: 3.2065 - mse: 3.2065\n",
      "Epoch 229/500\n",
      "2/2 - 0s - loss: 3.0246 - mse: 3.0246\n",
      "Epoch 230/500\n",
      "2/2 - 0s - loss: 2.6159 - mse: 2.6159\n",
      "Epoch 231/500\n",
      "2/2 - 0s - loss: 2.5200 - mse: 2.5200\n",
      "Epoch 232/500\n",
      "2/2 - 0s - loss: 2.5187 - mse: 2.5187\n",
      "Epoch 233/500\n",
      "2/2 - 0s - loss: 2.3519 - mse: 2.3519\n",
      "Epoch 234/500\n",
      "2/2 - 0s - loss: 3.7596 - mse: 3.7596\n",
      "Epoch 235/500\n",
      "2/2 - 0s - loss: 4.5290 - mse: 4.5290\n",
      "Epoch 236/500\n",
      "2/2 - 0s - loss: 3.9508 - mse: 3.9508\n",
      "Epoch 237/500\n",
      "2/2 - 0s - loss: 2.4759 - mse: 2.4759\n",
      "Epoch 238/500\n",
      "2/2 - 0s - loss: 3.1686 - mse: 3.1686\n",
      "Epoch 239/500\n",
      "2/2 - 0s - loss: 2.6915 - mse: 2.6915\n",
      "Epoch 240/500\n",
      "2/2 - 0s - loss: 1.6469 - mse: 1.6469\n",
      "Epoch 241/500\n",
      "2/2 - 0s - loss: 1.6002 - mse: 1.6002\n",
      "Epoch 242/500\n",
      "2/2 - 0s - loss: 1.6730 - mse: 1.6730\n",
      "Epoch 243/500\n",
      "2/2 - 0s - loss: 4.1305 - mse: 4.1305\n",
      "Epoch 244/500\n",
      "2/2 - 0s - loss: 5.5070 - mse: 5.5070\n",
      "Epoch 245/500\n",
      "2/2 - 0s - loss: 2.3541 - mse: 2.3541\n",
      "Epoch 246/500\n",
      "2/2 - 0s - loss: 2.7464 - mse: 2.7464\n",
      "Epoch 247/500\n",
      "2/2 - 0s - loss: 3.7229 - mse: 3.7229\n",
      "Epoch 248/500\n",
      "2/2 - 0s - loss: 3.6912 - mse: 3.6912\n",
      "Epoch 249/500\n",
      "2/2 - 0s - loss: 2.3952 - mse: 2.3952\n",
      "Epoch 250/500\n",
      "2/2 - 0s - loss: 2.4686 - mse: 2.4686\n",
      "Epoch 251/500\n",
      "2/2 - 0s - loss: 2.2637 - mse: 2.2637\n",
      "Epoch 252/500\n",
      "2/2 - 0s - loss: 1.6281 - mse: 1.6281\n",
      "Epoch 253/500\n",
      "2/2 - 0s - loss: 1.5733 - mse: 1.5733\n",
      "Epoch 254/500\n",
      "2/2 - 0s - loss: 2.2185 - mse: 2.2185\n",
      "Epoch 255/500\n",
      "2/2 - 0s - loss: 9.6726 - mse: 9.6726\n",
      "Epoch 256/500\n",
      "2/2 - 0s - loss: 3.3873 - mse: 3.3873\n",
      "Epoch 257/500\n",
      "2/2 - 0s - loss: 1.5602 - mse: 1.5602\n",
      "Epoch 258/500\n",
      "2/2 - 0s - loss: 2.1862 - mse: 2.1862\n",
      "Epoch 259/500\n",
      "2/2 - 0s - loss: 3.7277 - mse: 3.7277\n",
      "Epoch 260/500\n",
      "2/2 - 0s - loss: 3.5709 - mse: 3.5709\n",
      "Epoch 261/500\n",
      "2/2 - 0s - loss: 2.2129 - mse: 2.2129\n",
      "Epoch 262/500\n",
      "2/2 - 0s - loss: 1.5793 - mse: 1.5793\n",
      "Epoch 263/500\n",
      "2/2 - 0s - loss: 2.6095 - mse: 2.6095\n",
      "Epoch 264/500\n",
      "2/2 - 0s - loss: 4.6627 - mse: 4.6627\n",
      "Epoch 265/500\n",
      "2/2 - 0s - loss: 3.0342 - mse: 3.0342\n",
      "Epoch 266/500\n",
      "2/2 - 0s - loss: 3.8872 - mse: 3.8872\n",
      "Epoch 267/500\n",
      "2/2 - 0s - loss: 3.6512 - mse: 3.6512\n",
      "Epoch 268/500\n",
      "2/2 - 0s - loss: 2.8145 - mse: 2.8145\n",
      "Epoch 269/500\n",
      "2/2 - 0s - loss: 1.7316 - mse: 1.7316\n",
      "Epoch 270/500\n",
      "2/2 - 0s - loss: 3.0251 - mse: 3.0251\n",
      "Epoch 271/500\n",
      "2/2 - 0s - loss: 2.5088 - mse: 2.5088\n",
      "Epoch 272/500\n",
      "2/2 - 0s - loss: 1.5055 - mse: 1.5055\n",
      "Epoch 273/500\n",
      "2/2 - 0s - loss: 1.6188 - mse: 1.6188\n",
      "Epoch 274/500\n",
      "2/2 - 0s - loss: 1.7838 - mse: 1.7838\n",
      "Epoch 275/500\n",
      "2/2 - 0s - loss: 2.0879 - mse: 2.0879\n",
      "Epoch 276/500\n",
      "2/2 - 0s - loss: 3.0140 - mse: 3.0140\n",
      "Epoch 277/500\n",
      "2/2 - 0s - loss: 5.5730 - mse: 5.5730\n",
      "Epoch 278/500\n",
      "2/2 - 0s - loss: 4.1483 - mse: 4.1483\n",
      "Epoch 279/500\n",
      "2/2 - 0s - loss: 2.2924 - mse: 2.2924\n",
      "Epoch 280/500\n",
      "2/2 - 0s - loss: 1.7100 - mse: 1.7100\n",
      "Epoch 281/500\n",
      "2/2 - 0s - loss: 2.3162 - mse: 2.3162\n",
      "Epoch 282/500\n",
      "2/2 - 0s - loss: 2.0413 - mse: 2.0413\n",
      "Epoch 283/500\n",
      "2/2 - 0s - loss: 2.2684 - mse: 2.2684\n",
      "Epoch 284/500\n",
      "2/2 - 0s - loss: 2.3064 - mse: 2.3064\n",
      "Epoch 285/500\n",
      "2/2 - 0s - loss: 2.1441 - mse: 2.1441\n",
      "Epoch 286/500\n",
      "2/2 - 0s - loss: 4.4141 - mse: 4.4141\n",
      "Epoch 287/500\n",
      "2/2 - 0s - loss: 4.5661 - mse: 4.5661\n",
      "Epoch 288/500\n",
      "2/2 - 0s - loss: 4.1665 - mse: 4.1665\n",
      "Epoch 289/500\n",
      "2/2 - 0s - loss: 3.6382 - mse: 3.6382\n",
      "Epoch 290/500\n",
      "2/2 - 0s - loss: 1.8418 - mse: 1.8418\n",
      "Epoch 291/500\n",
      "2/2 - 0s - loss: 1.4769 - mse: 1.4769\n",
      "Epoch 292/500\n",
      "2/2 - 0s - loss: 3.4880 - mse: 3.4880\n",
      "Epoch 293/500\n",
      "2/2 - 0s - loss: 4.3402 - mse: 4.3402\n",
      "Epoch 294/500\n",
      "2/2 - 0s - loss: 1.7588 - mse: 1.7588\n",
      "Epoch 295/500\n",
      "2/2 - 0s - loss: 1.4519 - mse: 1.4519\n",
      "Epoch 296/500\n",
      "2/2 - 0s - loss: 1.5425 - mse: 1.5425\n",
      "Epoch 297/500\n",
      "2/2 - 0s - loss: 1.6391 - mse: 1.6391\n",
      "Epoch 298/500\n",
      "2/2 - 0s - loss: 2.9912 - mse: 2.9912\n",
      "Epoch 299/500\n",
      "2/2 - 0s - loss: 7.4789 - mse: 7.4789\n",
      "Epoch 300/500\n",
      "2/2 - 0s - loss: 1.9467 - mse: 1.9467\n",
      "Epoch 301/500\n",
      "2/2 - 0s - loss: 2.5013 - mse: 2.5013\n",
      "Epoch 302/500\n",
      "2/2 - 0s - loss: 4.6894 - mse: 4.6894\n",
      "Epoch 303/500\n",
      "2/2 - 0s - loss: 2.6893 - mse: 2.6893\n",
      "Epoch 304/500\n",
      "2/2 - 0s - loss: 1.7198 - mse: 1.7198\n",
      "Epoch 305/500\n",
      "2/2 - 0s - loss: 1.7753 - mse: 1.7753\n",
      "Epoch 306/500\n",
      "2/2 - 0s - loss: 1.5002 - mse: 1.5002\n",
      "Epoch 307/500\n",
      "2/2 - 0s - loss: 1.4942 - mse: 1.4942\n",
      "Epoch 308/500\n",
      "2/2 - 0s - loss: 1.4283 - mse: 1.4283\n",
      "Epoch 309/500\n",
      "2/2 - 0s - loss: 1.5112 - mse: 1.5112\n",
      "Epoch 310/500\n",
      "2/2 - 0s - loss: 3.9106 - mse: 3.9106\n",
      "Epoch 311/500\n",
      "2/2 - 0s - loss: 9.2620 - mse: 9.2620\n",
      "Epoch 312/500\n",
      "2/2 - 0s - loss: 3.3049 - mse: 3.3049\n",
      "Epoch 313/500\n",
      "2/2 - 0s - loss: 1.4837 - mse: 1.4837\n",
      "Epoch 314/500\n",
      "2/2 - 0s - loss: 1.4137 - mse: 1.4137\n",
      "Epoch 315/500\n",
      "2/2 - 0s - loss: 1.5180 - mse: 1.5180\n",
      "Epoch 316/500\n",
      "2/2 - 0s - loss: 1.8228 - mse: 1.8228\n",
      "Epoch 317/500\n",
      "2/2 - 0s - loss: 2.1964 - mse: 2.1964\n",
      "Epoch 318/500\n",
      "2/2 - 0s - loss: 2.4951 - mse: 2.4951\n",
      "Epoch 319/500\n",
      "2/2 - 0s - loss: 3.3842 - mse: 3.3842\n",
      "Epoch 320/500\n",
      "2/2 - 0s - loss: 3.8114 - mse: 3.8114\n",
      "Epoch 321/500\n",
      "2/2 - 0s - loss: 2.2386 - mse: 2.2386\n",
      "Epoch 322/500\n",
      "2/2 - 0s - loss: 1.4098 - mse: 1.4098\n",
      "Epoch 323/500\n",
      "2/2 - 0s - loss: 2.0981 - mse: 2.0981\n",
      "Epoch 324/500\n",
      "2/2 - 0s - loss: 2.9680 - mse: 2.9680\n",
      "Epoch 325/500\n",
      "2/2 - 0s - loss: 2.7703 - mse: 2.7703\n",
      "Epoch 326/500\n",
      "2/2 - 0s - loss: 3.5796 - mse: 3.5796\n",
      "Epoch 327/500\n",
      "2/2 - 0s - loss: 3.9144 - mse: 3.9144\n",
      "Epoch 328/500\n",
      "2/2 - 0s - loss: 2.5190 - mse: 2.5190\n",
      "Epoch 329/500\n",
      "2/2 - 0s - loss: 1.7153 - mse: 1.7153\n",
      "Epoch 330/500\n",
      "2/2 - 0s - loss: 1.3877 - mse: 1.3877\n",
      "Epoch 331/500\n",
      "2/2 - 0s - loss: 1.9214 - mse: 1.9214\n",
      "Epoch 332/500\n",
      "2/2 - 0s - loss: 3.7313 - mse: 3.7313\n",
      "Epoch 333/500\n",
      "2/2 - 0s - loss: 3.9701 - mse: 3.9701\n",
      "Epoch 334/500\n",
      "2/2 - 0s - loss: 1.4763 - mse: 1.4763\n",
      "Epoch 335/500\n",
      "2/2 - 0s - loss: 1.5056 - mse: 1.5056\n",
      "Epoch 336/500\n",
      "2/2 - 0s - loss: 2.0398 - mse: 2.0398\n",
      "Epoch 337/500\n",
      "2/2 - 0s - loss: 1.5027 - mse: 1.5027\n",
      "Epoch 338/500\n",
      "2/2 - 0s - loss: 1.5502 - mse: 1.5502\n",
      "Epoch 339/500\n",
      "2/2 - 0s - loss: 2.8514 - mse: 2.8514\n",
      "Epoch 340/500\n",
      "2/2 - 0s - loss: 5.3431 - mse: 5.3431\n",
      "Epoch 341/500\n",
      "2/2 - 0s - loss: 4.3299 - mse: 4.3299\n",
      "Epoch 342/500\n",
      "2/2 - 0s - loss: 3.6424 - mse: 3.6424\n",
      "Epoch 343/500\n",
      "2/2 - 0s - loss: 1.5093 - mse: 1.5093\n",
      "Epoch 344/500\n",
      "2/2 - 0s - loss: 1.5378 - mse: 1.5378\n",
      "Epoch 345/500\n",
      "2/2 - 0s - loss: 1.6970 - mse: 1.6970\n",
      "Epoch 346/500\n",
      "2/2 - 0s - loss: 1.7358 - mse: 1.7358\n",
      "Epoch 347/500\n",
      "2/2 - 0s - loss: 2.2320 - mse: 2.2320\n",
      "Epoch 348/500\n",
      "2/2 - 0s - loss: 4.8934 - mse: 4.8934\n",
      "Epoch 349/500\n",
      "2/2 - 0s - loss: 5.7813 - mse: 5.7813\n",
      "Epoch 350/500\n",
      "2/2 - 0s - loss: 1.8310 - mse: 1.8310\n",
      "Epoch 351/500\n",
      "2/2 - 0s - loss: 1.4249 - mse: 1.4249\n",
      "Epoch 352/500\n",
      "2/2 - 0s - loss: 1.4217 - mse: 1.4217\n",
      "Epoch 353/500\n",
      "2/2 - 0s - loss: 1.4346 - mse: 1.4346\n",
      "Epoch 354/500\n",
      "2/2 - 0s - loss: 2.4217 - mse: 2.4217\n",
      "Epoch 355/500\n",
      "2/2 - 0s - loss: 4.3844 - mse: 4.3844\n",
      "Epoch 356/500\n",
      "2/2 - 0s - loss: 3.1784 - mse: 3.1784\n",
      "Epoch 357/500\n",
      "2/2 - 0s - loss: 2.6062 - mse: 2.6062\n",
      "Epoch 358/500\n",
      "2/2 - 0s - loss: 1.9538 - mse: 1.9538\n",
      "Epoch 359/500\n",
      "2/2 - 0s - loss: 1.5293 - mse: 1.5293\n",
      "Epoch 360/500\n",
      "2/2 - 0s - loss: 1.9708 - mse: 1.9708\n",
      "Epoch 361/500\n",
      "2/2 - 0s - loss: 2.3486 - mse: 2.3486\n",
      "Epoch 362/500\n",
      "2/2 - 0s - loss: 2.1678 - mse: 2.1678\n",
      "Epoch 363/500\n",
      "2/2 - 0s - loss: 1.6564 - mse: 1.6564\n",
      "Epoch 364/500\n",
      "2/2 - 0s - loss: 2.0126 - mse: 2.0126\n",
      "Epoch 365/500\n",
      "2/2 - 0s - loss: 5.9994 - mse: 5.9994\n",
      "Epoch 366/500\n",
      "2/2 - 0s - loss: 4.7779 - mse: 4.7779\n",
      "Epoch 367/500\n",
      "2/2 - 0s - loss: 1.5746 - mse: 1.5746\n",
      "Epoch 368/500\n",
      "2/2 - 0s - loss: 1.3532 - mse: 1.3532\n",
      "Epoch 369/500\n",
      "2/2 - 0s - loss: 1.3645 - mse: 1.3645\n",
      "Epoch 370/500\n",
      "2/2 - 0s - loss: 1.3327 - mse: 1.3327\n",
      "Epoch 371/500\n",
      "2/2 - 0s - loss: 2.8621 - mse: 2.8621\n",
      "Epoch 372/500\n",
      "2/2 - 0s - loss: 3.7319 - mse: 3.7319\n",
      "Epoch 373/500\n",
      "2/2 - 0s - loss: 1.6892 - mse: 1.6892\n",
      "Epoch 374/500\n",
      "2/2 - 0s - loss: 4.0716 - mse: 4.0716\n",
      "Epoch 375/500\n",
      "2/2 - 0s - loss: 3.8862 - mse: 3.8862\n",
      "Epoch 376/500\n",
      "2/2 - 0s - loss: 2.1993 - mse: 2.1993\n",
      "Epoch 377/500\n",
      "2/2 - 0s - loss: 2.1177 - mse: 2.1177\n",
      "Epoch 378/500\n",
      "2/2 - 0s - loss: 1.7405 - mse: 1.7405\n",
      "Epoch 379/500\n",
      "2/2 - 0s - loss: 2.3412 - mse: 2.3412\n",
      "Epoch 380/500\n",
      "2/2 - 0s - loss: 3.9822 - mse: 3.9822\n",
      "Epoch 381/500\n",
      "2/2 - 0s - loss: 4.8916 - mse: 4.8916\n",
      "Epoch 382/500\n",
      "2/2 - 0s - loss: 2.6417 - mse: 2.6417\n",
      "Epoch 383/500\n",
      "2/2 - 0s - loss: 1.8801 - mse: 1.8801\n",
      "Epoch 384/500\n",
      "2/2 - 0s - loss: 1.4087 - mse: 1.4087\n",
      "Epoch 385/500\n",
      "2/2 - 0s - loss: 1.3063 - mse: 1.3063\n",
      "Epoch 386/500\n",
      "2/2 - 0s - loss: 1.3464 - mse: 1.3464\n",
      "Epoch 387/500\n",
      "2/2 - 0s - loss: 1.9166 - mse: 1.9166\n",
      "Epoch 388/500\n",
      "2/2 - 0s - loss: 4.3068 - mse: 4.3068\n",
      "Epoch 389/500\n",
      "2/2 - 0s - loss: 4.5441 - mse: 4.5441\n",
      "Epoch 390/500\n",
      "2/2 - 0s - loss: 3.0070 - mse: 3.0070\n",
      "Epoch 391/500\n",
      "2/2 - 0s - loss: 2.3737 - mse: 2.3737\n",
      "Epoch 392/500\n",
      "2/2 - 0s - loss: 2.0505 - mse: 2.0505\n",
      "Epoch 393/500\n",
      "2/2 - 0s - loss: 2.2657 - mse: 2.2657\n",
      "Epoch 394/500\n",
      "2/2 - 0s - loss: 2.6628 - mse: 2.6628\n",
      "Epoch 395/500\n",
      "2/2 - 0s - loss: 3.9185 - mse: 3.9185\n",
      "Epoch 396/500\n",
      "2/2 - 0s - loss: 3.0622 - mse: 3.0622\n",
      "Epoch 397/500\n",
      "2/2 - 0s - loss: 1.2899 - mse: 1.2899\n",
      "Epoch 398/500\n",
      "2/2 - 0s - loss: 1.5040 - mse: 1.5040\n",
      "Epoch 399/500\n",
      "2/2 - 0s - loss: 1.3590 - mse: 1.3590\n",
      "Epoch 400/500\n",
      "2/2 - 0s - loss: 1.4659 - mse: 1.4659\n",
      "Epoch 401/500\n",
      "2/2 - 0s - loss: 2.0245 - mse: 2.0245\n",
      "Epoch 402/500\n",
      "2/2 - 0s - loss: 6.6715 - mse: 6.6715\n",
      "Epoch 403/500\n",
      "2/2 - 0s - loss: 4.2736 - mse: 4.2736\n",
      "Epoch 404/500\n",
      "2/2 - 0s - loss: 1.7277 - mse: 1.7277\n",
      "Epoch 405/500\n",
      "2/2 - 0s - loss: 1.2987 - mse: 1.2987\n",
      "Epoch 406/500\n",
      "2/2 - 0s - loss: 1.2840 - mse: 1.2840\n",
      "Epoch 407/500\n",
      "2/2 - 0s - loss: 1.3497 - mse: 1.3497\n",
      "Epoch 408/500\n",
      "2/2 - 0s - loss: 1.3748 - mse: 1.3748\n",
      "Epoch 409/500\n",
      "2/2 - 0s - loss: 2.4676 - mse: 2.4676\n",
      "Epoch 410/500\n",
      "2/2 - 0s - loss: 4.2305 - mse: 4.2305\n",
      "Epoch 411/500\n",
      "2/2 - 0s - loss: 3.0183 - mse: 3.0183\n",
      "Epoch 412/500\n",
      "2/2 - 0s - loss: 2.7153 - mse: 2.7153\n",
      "Epoch 413/500\n",
      "2/2 - 0s - loss: 1.5658 - mse: 1.5658\n",
      "Epoch 414/500\n",
      "2/2 - 0s - loss: 1.6002 - mse: 1.6002\n",
      "Epoch 415/500\n",
      "2/2 - 0s - loss: 2.1914 - mse: 2.1914\n",
      "Epoch 416/500\n",
      "2/2 - 0s - loss: 5.9668 - mse: 5.9668\n",
      "Epoch 417/500\n",
      "2/2 - 0s - loss: 3.7731 - mse: 3.7731\n",
      "Epoch 418/500\n",
      "2/2 - 0s - loss: 1.6717 - mse: 1.6717\n",
      "Epoch 419/500\n",
      "2/2 - 0s - loss: 1.4302 - mse: 1.4302\n",
      "Epoch 420/500\n",
      "2/2 - 0s - loss: 1.2680 - mse: 1.2680\n",
      "Epoch 421/500\n",
      "2/2 - 0s - loss: 1.3797 - mse: 1.3797\n",
      "Epoch 422/500\n",
      "2/2 - 0s - loss: 2.5541 - mse: 2.5541\n",
      "Epoch 423/500\n",
      "2/2 - 0s - loss: 6.0335 - mse: 6.0335\n",
      "Epoch 424/500\n",
      "2/2 - 0s - loss: 3.5990 - mse: 3.5990\n",
      "Epoch 425/500\n",
      "2/2 - 0s - loss: 1.4126 - mse: 1.4126\n",
      "Epoch 426/500\n",
      "2/2 - 0s - loss: 1.2465 - mse: 1.2465\n",
      "Epoch 427/500\n",
      "2/2 - 0s - loss: 1.2420 - mse: 1.2420\n",
      "Epoch 428/500\n",
      "2/2 - 0s - loss: 1.4400 - mse: 1.4400\n",
      "Epoch 429/500\n",
      "2/2 - 0s - loss: 1.6736 - mse: 1.6736\n",
      "Epoch 430/500\n",
      "2/2 - 0s - loss: 1.5854 - mse: 1.5854\n",
      "Epoch 431/500\n",
      "2/2 - 0s - loss: 2.3275 - mse: 2.3275\n",
      "Epoch 432/500\n",
      "2/2 - 0s - loss: 5.8440 - mse: 5.8440\n",
      "Epoch 433/500\n",
      "2/2 - 0s - loss: 3.8263 - mse: 3.8263\n",
      "Epoch 434/500\n",
      "2/2 - 0s - loss: 1.2574 - mse: 1.2574\n",
      "Epoch 435/500\n",
      "2/2 - 0s - loss: 1.3984 - mse: 1.3984\n",
      "Epoch 436/500\n",
      "2/2 - 0s - loss: 1.4306 - mse: 1.4306\n",
      "Epoch 437/500\n",
      "2/2 - 0s - loss: 1.4383 - mse: 1.4383\n",
      "Epoch 438/500\n",
      "2/2 - 0s - loss: 2.4840 - mse: 2.4840\n",
      "Epoch 439/500\n",
      "2/2 - 0s - loss: 4.3433 - mse: 4.3433\n",
      "Epoch 440/500\n",
      "2/2 - 0s - loss: 3.7255 - mse: 3.7255\n",
      "Epoch 441/500\n",
      "2/2 - 0s - loss: 1.7572 - mse: 1.7572\n",
      "Epoch 442/500\n",
      "2/2 - 0s - loss: 1.2510 - mse: 1.2510\n",
      "Epoch 443/500\n",
      "2/2 - 0s - loss: 1.4305 - mse: 1.4305\n",
      "Epoch 444/500\n",
      "2/2 - 0s - loss: 2.4886 - mse: 2.4886\n",
      "Epoch 445/500\n",
      "2/2 - 0s - loss: 4.1262 - mse: 4.1262\n",
      "Epoch 446/500\n",
      "2/2 - 0s - loss: 3.4566 - mse: 3.4566\n",
      "Epoch 447/500\n",
      "2/2 - 0s - loss: 2.7005 - mse: 2.7005\n",
      "Epoch 448/500\n",
      "2/2 - 0s - loss: 3.0340 - mse: 3.0340\n",
      "Epoch 449/500\n",
      "2/2 - 0s - loss: 2.6415 - mse: 2.6415\n",
      "Epoch 450/500\n",
      "2/2 - 0s - loss: 2.5525 - mse: 2.5525\n",
      "Epoch 451/500\n",
      "2/2 - 0s - loss: 3.1371 - mse: 3.1371\n",
      "Epoch 452/500\n",
      "2/2 - 0s - loss: 3.4832 - mse: 3.4832\n",
      "Epoch 453/500\n",
      "2/2 - 0s - loss: 1.4324 - mse: 1.4324\n",
      "Epoch 454/500\n",
      "2/2 - 0s - loss: 1.2769 - mse: 1.2769\n",
      "Epoch 455/500\n",
      "2/2 - 0s - loss: 1.7268 - mse: 1.7268\n",
      "Epoch 456/500\n",
      "2/2 - 0s - loss: 3.2769 - mse: 3.2769\n",
      "Epoch 457/500\n",
      "2/2 - 0s - loss: 4.0685 - mse: 4.0685\n",
      "Epoch 458/500\n",
      "2/2 - 0s - loss: 1.7878 - mse: 1.7878\n",
      "Epoch 459/500\n",
      "2/2 - 0s - loss: 1.2462 - mse: 1.2462\n",
      "Epoch 460/500\n",
      "2/2 - 0s - loss: 2.2281 - mse: 2.2281\n",
      "Epoch 461/500\n",
      "2/2 - 0s - loss: 2.4065 - mse: 2.4065\n",
      "Epoch 462/500\n",
      "2/2 - 0s - loss: 2.2556 - mse: 2.2556\n",
      "Epoch 463/500\n",
      "2/2 - 0s - loss: 2.1220 - mse: 2.1220\n",
      "Epoch 464/500\n",
      "2/2 - 0s - loss: 2.8694 - mse: 2.8694\n",
      "Epoch 465/500\n",
      "2/2 - 0s - loss: 1.9353 - mse: 1.9353\n",
      "Epoch 466/500\n",
      "2/2 - 0s - loss: 1.2326 - mse: 1.2326\n",
      "Epoch 467/500\n",
      "2/2 - 0s - loss: 1.4831 - mse: 1.4831\n",
      "Epoch 468/500\n",
      "2/2 - 0s - loss: 1.9263 - mse: 1.9263\n",
      "Epoch 469/500\n",
      "2/2 - 0s - loss: 1.5297 - mse: 1.5297\n",
      "Epoch 470/500\n",
      "2/2 - 0s - loss: 2.9288 - mse: 2.9288\n",
      "Epoch 471/500\n",
      "2/2 - 0s - loss: 5.2221 - mse: 5.2221\n",
      "Epoch 472/500\n",
      "2/2 - 0s - loss: 2.4868 - mse: 2.4868\n",
      "Epoch 473/500\n",
      "2/2 - 0s - loss: 1.9409 - mse: 1.9409\n",
      "Epoch 474/500\n",
      "2/2 - 0s - loss: 2.3478 - mse: 2.3478\n",
      "Epoch 475/500\n",
      "2/2 - 0s - loss: 3.5568 - mse: 3.5568\n",
      "Epoch 476/500\n",
      "2/2 - 0s - loss: 4.2406 - mse: 4.2406\n",
      "Epoch 477/500\n",
      "2/2 - 0s - loss: 2.7863 - mse: 2.7863\n",
      "Epoch 478/500\n",
      "2/2 - 0s - loss: 1.2612 - mse: 1.2612\n",
      "Epoch 479/500\n",
      "2/2 - 0s - loss: 1.7369 - mse: 1.7369\n",
      "Epoch 480/500\n",
      "2/2 - 0s - loss: 1.2554 - mse: 1.2554\n",
      "Epoch 481/500\n",
      "2/2 - 0s - loss: 2.4952 - mse: 2.4952\n",
      "Epoch 482/500\n",
      "2/2 - 0s - loss: 3.1628 - mse: 3.1628\n",
      "Epoch 483/500\n",
      "2/2 - 0s - loss: 1.5861 - mse: 1.5861\n",
      "Epoch 484/500\n",
      "2/2 - 0s - loss: 1.1986 - mse: 1.1986\n",
      "Epoch 485/500\n",
      "2/2 - 0s - loss: 3.0163 - mse: 3.0163\n",
      "Epoch 486/500\n",
      "2/2 - 0s - loss: 5.5209 - mse: 5.5209\n",
      "Epoch 487/500\n",
      "2/2 - 0s - loss: 2.3760 - mse: 2.3760\n",
      "Epoch 488/500\n",
      "2/2 - 0s - loss: 1.4576 - mse: 1.4576\n",
      "Epoch 489/500\n",
      "2/2 - 0s - loss: 1.6532 - mse: 1.6532\n",
      "Epoch 490/500\n",
      "2/2 - 0s - loss: 1.5325 - mse: 1.5325\n",
      "Epoch 491/500\n",
      "2/2 - 0s - loss: 1.1722 - mse: 1.1722\n",
      "Epoch 492/500\n",
      "2/2 - 0s - loss: 1.1640 - mse: 1.1640\n",
      "Epoch 493/500\n",
      "2/2 - 0s - loss: 1.8137 - mse: 1.8137\n",
      "Epoch 494/500\n",
      "2/2 - 0s - loss: 5.8596 - mse: 5.8596\n",
      "Epoch 495/500\n",
      "2/2 - 0s - loss: 4.1617 - mse: 4.1617\n",
      "Epoch 496/500\n",
      "2/2 - 0s - loss: 1.4041 - mse: 1.4041\n",
      "Epoch 497/500\n",
      "2/2 - 0s - loss: 1.4050 - mse: 1.4050\n",
      "Epoch 498/500\n",
      "2/2 - 0s - loss: 1.4794 - mse: 1.4794\n",
      "Epoch 499/500\n",
      "2/2 - 0s - loss: 1.6425 - mse: 1.6425\n",
      "Epoch 500/500\n",
      "2/2 - 0s - loss: 3.0761 - mse: 3.0761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16af002b0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.1013725 , -0.10137112, -0.10146169],\n",
       "        [-0.21688122, -0.21686642, -0.21711478],\n",
       "        [-0.15532972, -0.15532176, -0.15549019],\n",
       "        [-0.23704672, -0.23703481, -0.23724623],\n",
       "        [ 0.00144791,  0.00137385,  0.00224039],\n",
       "        [-0.00758032, -0.00764291, -0.00692421]], dtype=float32),\n",
       " array([-0.09405913, -0.09409671, -0.09369185], dtype=float32),\n",
       " array([[-0.7906922 ],\n",
       "        [-0.9247624 ],\n",
       "        [-0.30857703]], dtype=float32),\n",
       " array([0.0943292], dtype=float32)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:06:36.036010: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>16.924698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>15.842933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.270544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>20.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.968329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             16.924698\n",
       "AK       18.1             15.842933\n",
       "AZ       18.6             16.270544\n",
       "AR       22.4             20.094500\n",
       "CA       12.0             10.968329"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Dense(units=12, input_dim=8, kernel_initializer='zeros'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/3015801051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 6)\n"
     ]
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "AL       18.8     7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.6498522 ],\n",
       "        [ 0.01582366],\n",
       "        [ 0.0313558 ],\n",
       "        [-0.18843806],\n",
       "        [ 0.01710051],\n",
       "        [ 0.5997405 ],\n",
       "        [ 0.2519806 ],\n",
       "        [-0.14528346],\n",
       "        [ 0.52136314],\n",
       "        [-0.14928257],\n",
       "        [ 0.3188057 ],\n",
       "        [ 0.2842927 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/3665604266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 6)\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.6498522 ],\n",
       "        [ 0.01582366],\n",
       "        [ 0.0313558 ],\n",
       "        [-0.18843806],\n",
       "        [ 0.01710051],\n",
       "        [ 0.5997405 ],\n",
       "        [ 0.2519806 ],\n",
       "        [-0.14528346],\n",
       "        [ 0.52136314],\n",
       "        [-0.14928257],\n",
       "        [ 0.3188057 ],\n",
       "        [ 0.2842927 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/2310779427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/sota/miniforge3/envs/dl/lib/python3.9/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_14 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 6)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>16.924698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>15.842933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.270544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>20.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.968329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             16.924698\n",
       "AK       18.1             15.842933\n",
       "AZ       18.6             16.270544\n",
       "AR       22.4             20.094500\n",
       "CA       12.0             10.968329"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 - 0s - loss: 61674.1055 - mse: 61674.1055\n",
      "Epoch 2/500\n",
      "2/2 - 0s - loss: 54199.6289 - mse: 54199.6289\n",
      "Epoch 3/500\n",
      "2/2 - 0s - loss: 49451.6016 - mse: 49451.6016\n",
      "Epoch 4/500\n",
      "2/2 - 0s - loss: 45709.4727 - mse: 45709.4727\n",
      "Epoch 5/500\n",
      "2/2 - 0s - loss: 42553.0117 - mse: 42553.0117\n",
      "Epoch 6/500\n",
      "2/2 - 0s - loss: 39694.8672 - mse: 39694.8672\n",
      "Epoch 7/500\n",
      "2/2 - 0s - loss: 37015.6641 - mse: 37015.6641\n",
      "Epoch 8/500\n",
      "2/2 - 0s - loss: 34577.3164 - mse: 34577.3125\n",
      "Epoch 9/500\n",
      "2/2 - 0s - loss: 32354.6465 - mse: 32354.6465\n",
      "Epoch 10/500\n",
      "2/2 - 0s - loss: 30285.3262 - mse: 30285.3262\n",
      "Epoch 11/500\n",
      "2/2 - 0s - loss: 28314.7266 - mse: 28314.7266\n",
      "Epoch 12/500\n",
      "2/2 - 0s - loss: 26465.5098 - mse: 26465.5098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:06:54.297063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "2/2 - 0s - loss: 24700.5195 - mse: 24700.5195\n",
      "Epoch 14/500\n",
      "2/2 - 0s - loss: 23022.8418 - mse: 23022.8418\n",
      "Epoch 15/500\n",
      "2/2 - 0s - loss: 21450.4648 - mse: 21450.4648\n",
      "Epoch 16/500\n",
      "2/2 - 0s - loss: 19953.2988 - mse: 19953.2988\n",
      "Epoch 17/500\n",
      "2/2 - 0s - loss: 18521.0059 - mse: 18521.0059\n",
      "Epoch 18/500\n",
      "2/2 - 0s - loss: 17129.0840 - mse: 17129.0840\n",
      "Epoch 19/500\n",
      "2/2 - 0s - loss: 15809.9102 - mse: 15809.9102\n",
      "Epoch 20/500\n",
      "2/2 - 0s - loss: 14583.2295 - mse: 14583.2295\n",
      "Epoch 21/500\n",
      "2/2 - 0s - loss: 13412.4609 - mse: 13412.4609\n",
      "Epoch 22/500\n",
      "2/2 - 0s - loss: 12292.2451 - mse: 12292.2451\n",
      "Epoch 23/500\n",
      "2/2 - 0s - loss: 11213.1484 - mse: 11213.1484\n",
      "Epoch 24/500\n",
      "2/2 - 0s - loss: 10195.9092 - mse: 10195.9092\n",
      "Epoch 25/500\n",
      "2/2 - 0s - loss: 9265.6582 - mse: 9265.6582\n",
      "Epoch 26/500\n",
      "2/2 - 0s - loss: 8388.5625 - mse: 8388.5625\n",
      "Epoch 27/500\n",
      "2/2 - 0s - loss: 7545.3501 - mse: 7545.3501\n",
      "Epoch 28/500\n",
      "2/2 - 0s - loss: 6762.2080 - mse: 6762.2080\n",
      "Epoch 29/500\n",
      "2/2 - 0s - loss: 6050.2197 - mse: 6050.2197\n",
      "Epoch 30/500\n",
      "2/2 - 0s - loss: 5374.6841 - mse: 5374.6841\n",
      "Epoch 31/500\n",
      "2/2 - 0s - loss: 4738.9771 - mse: 4738.9771\n",
      "Epoch 32/500\n",
      "2/2 - 0s - loss: 4171.9033 - mse: 4171.9033\n",
      "Epoch 33/500\n",
      "2/2 - 0s - loss: 3652.4001 - mse: 3652.4001\n",
      "Epoch 34/500\n",
      "2/2 - 0s - loss: 3163.5076 - mse: 3163.5076\n",
      "Epoch 35/500\n",
      "2/2 - 0s - loss: 2724.3721 - mse: 2724.3721\n",
      "Epoch 36/500\n",
      "2/2 - 0s - loss: 2329.3386 - mse: 2329.3386\n",
      "Epoch 37/500\n",
      "2/2 - 0s - loss: 1977.2682 - mse: 1977.2682\n",
      "Epoch 38/500\n",
      "2/2 - 0s - loss: 1666.0732 - mse: 1666.0732\n",
      "Epoch 39/500\n",
      "2/2 - 0s - loss: 1393.2545 - mse: 1393.2545\n",
      "Epoch 40/500\n",
      "2/2 - 0s - loss: 1155.1768 - mse: 1155.1768\n",
      "Epoch 41/500\n",
      "2/2 - 0s - loss: 953.7952 - mse: 953.7952\n",
      "Epoch 42/500\n",
      "2/2 - 0s - loss: 786.0607 - mse: 786.0607\n",
      "Epoch 43/500\n",
      "2/2 - 0s - loss: 643.5798 - mse: 643.5798\n",
      "Epoch 44/500\n",
      "2/2 - 0s - loss: 525.8705 - mse: 525.8705\n",
      "Epoch 45/500\n",
      "2/2 - 0s - loss: 433.1189 - mse: 433.1189\n",
      "Epoch 46/500\n",
      "2/2 - 0s - loss: 359.4237 - mse: 359.4237\n",
      "Epoch 47/500\n",
      "2/2 - 0s - loss: 301.5655 - mse: 301.5655\n",
      "Epoch 48/500\n",
      "2/2 - 0s - loss: 256.4215 - mse: 256.4215\n",
      "Epoch 49/500\n",
      "2/2 - 0s - loss: 224.6264 - mse: 224.6264\n",
      "Epoch 50/500\n",
      "2/2 - 0s - loss: 202.0349 - mse: 202.0349\n",
      "Epoch 51/500\n",
      "2/2 - 0s - loss: 191.0567 - mse: 191.0567\n",
      "Epoch 52/500\n",
      "2/2 - 0s - loss: 184.7809 - mse: 184.7809\n",
      "Epoch 53/500\n",
      "2/2 - 0s - loss: 180.5508 - mse: 180.5508\n",
      "Epoch 54/500\n",
      "2/2 - 0s - loss: 178.3429 - mse: 178.3429\n",
      "Epoch 55/500\n",
      "2/2 - 0s - loss: 175.9878 - mse: 175.9878\n",
      "Epoch 56/500\n",
      "2/2 - 0s - loss: 177.6709 - mse: 177.6709\n",
      "Epoch 57/500\n",
      "2/2 - 0s - loss: 176.0347 - mse: 176.0347\n",
      "Epoch 58/500\n",
      "2/2 - 0s - loss: 175.1032 - mse: 175.1032\n",
      "Epoch 59/500\n",
      "2/2 - 0s - loss: 174.3150 - mse: 174.3150\n",
      "Epoch 60/500\n",
      "2/2 - 0s - loss: 174.1084 - mse: 174.1084\n",
      "Epoch 61/500\n",
      "2/2 - 0s - loss: 174.4170 - mse: 174.4170\n",
      "Epoch 62/500\n",
      "2/2 - 0s - loss: 173.3748 - mse: 173.3748\n",
      "Epoch 63/500\n",
      "2/2 - 0s - loss: 177.0785 - mse: 177.0785\n",
      "Epoch 64/500\n",
      "2/2 - 0s - loss: 177.1653 - mse: 177.1653\n",
      "Epoch 65/500\n",
      "2/2 - 0s - loss: 172.9165 - mse: 172.9165\n",
      "Epoch 66/500\n",
      "2/2 - 0s - loss: 172.9527 - mse: 172.9527\n",
      "Epoch 67/500\n",
      "2/2 - 0s - loss: 172.0159 - mse: 172.0159\n",
      "Epoch 68/500\n",
      "2/2 - 0s - loss: 176.2147 - mse: 176.2147\n",
      "Epoch 69/500\n",
      "2/2 - 0s - loss: 175.2033 - mse: 175.2033\n",
      "Epoch 70/500\n",
      "2/2 - 0s - loss: 168.9915 - mse: 168.9915\n",
      "Epoch 71/500\n",
      "2/2 - 0s - loss: 166.2860 - mse: 166.2860\n",
      "Epoch 72/500\n",
      "2/2 - 0s - loss: 165.8467 - mse: 165.8467\n",
      "Epoch 73/500\n",
      "2/2 - 0s - loss: 168.6153 - mse: 168.6153\n",
      "Epoch 74/500\n",
      "2/2 - 0s - loss: 165.8595 - mse: 165.8595\n",
      "Epoch 75/500\n",
      "2/2 - 0s - loss: 165.6399 - mse: 165.6399\n",
      "Epoch 76/500\n",
      "2/2 - 0s - loss: 162.5673 - mse: 162.5673\n",
      "Epoch 77/500\n",
      "2/2 - 0s - loss: 161.3794 - mse: 161.3794\n",
      "Epoch 78/500\n",
      "2/2 - 0s - loss: 161.3773 - mse: 161.3773\n",
      "Epoch 79/500\n",
      "2/2 - 0s - loss: 159.8691 - mse: 159.8691\n",
      "Epoch 80/500\n",
      "2/2 - 0s - loss: 156.9061 - mse: 156.9061\n",
      "Epoch 81/500\n",
      "2/2 - 0s - loss: 156.5497 - mse: 156.5497\n",
      "Epoch 82/500\n",
      "2/2 - 0s - loss: 155.3275 - mse: 155.3275\n",
      "Epoch 83/500\n",
      "2/2 - 0s - loss: 153.1933 - mse: 153.1933\n",
      "Epoch 84/500\n",
      "2/2 - 0s - loss: 154.7346 - mse: 154.7346\n",
      "Epoch 85/500\n",
      "2/2 - 0s - loss: 166.2183 - mse: 166.2183\n",
      "Epoch 86/500\n",
      "2/2 - 0s - loss: 164.9792 - mse: 164.9792\n",
      "Epoch 87/500\n",
      "2/2 - 0s - loss: 157.9561 - mse: 157.9561\n",
      "Epoch 88/500\n",
      "2/2 - 0s - loss: 155.3372 - mse: 155.3372\n",
      "Epoch 89/500\n",
      "2/2 - 0s - loss: 160.0176 - mse: 160.0176\n",
      "Epoch 90/500\n",
      "2/2 - 0s - loss: 154.5765 - mse: 154.5765\n",
      "Epoch 91/500\n",
      "2/2 - 0s - loss: 148.1133 - mse: 148.1133\n",
      "Epoch 92/500\n",
      "2/2 - 0s - loss: 143.7108 - mse: 143.7108\n",
      "Epoch 93/500\n",
      "2/2 - 0s - loss: 142.6095 - mse: 142.6095\n",
      "Epoch 94/500\n",
      "2/2 - 0s - loss: 142.2799 - mse: 142.2799\n",
      "Epoch 95/500\n",
      "2/2 - 0s - loss: 141.3540 - mse: 141.3540\n",
      "Epoch 96/500\n",
      "2/2 - 0s - loss: 140.9066 - mse: 140.9066\n",
      "Epoch 97/500\n",
      "2/2 - 0s - loss: 152.7106 - mse: 152.7106\n",
      "Epoch 98/500\n",
      "2/2 - 0s - loss: 145.7925 - mse: 145.7925\n",
      "Epoch 99/500\n",
      "2/2 - 0s - loss: 141.9367 - mse: 141.9367\n",
      "Epoch 100/500\n",
      "2/2 - 0s - loss: 144.0398 - mse: 144.0398\n",
      "Epoch 101/500\n",
      "2/2 - 0s - loss: 139.0164 - mse: 139.0164\n",
      "Epoch 102/500\n",
      "2/2 - 0s - loss: 133.7309 - mse: 133.7309\n",
      "Epoch 103/500\n",
      "2/2 - 0s - loss: 134.2786 - mse: 134.2786\n",
      "Epoch 104/500\n",
      "2/2 - 0s - loss: 135.0905 - mse: 135.0905\n",
      "Epoch 105/500\n",
      "2/2 - 0s - loss: 145.4780 - mse: 145.4780\n",
      "Epoch 106/500\n",
      "2/2 - 0s - loss: 136.8908 - mse: 136.8908\n",
      "Epoch 107/500\n",
      "2/2 - 0s - loss: 129.4177 - mse: 129.4177\n",
      "Epoch 108/500\n",
      "2/2 - 0s - loss: 128.2943 - mse: 128.2943\n",
      "Epoch 109/500\n",
      "2/2 - 0s - loss: 129.8149 - mse: 129.8149\n",
      "Epoch 110/500\n",
      "2/2 - 0s - loss: 126.3531 - mse: 126.3531\n",
      "Epoch 111/500\n",
      "2/2 - 0s - loss: 130.5987 - mse: 130.5987\n",
      "Epoch 112/500\n",
      "2/2 - 0s - loss: 127.8297 - mse: 127.8297\n",
      "Epoch 113/500\n",
      "2/2 - 0s - loss: 123.3420 - mse: 123.3420\n",
      "Epoch 114/500\n",
      "2/2 - 0s - loss: 133.9095 - mse: 133.9095\n",
      "Epoch 115/500\n",
      "2/2 - 0s - loss: 127.6736 - mse: 127.6736\n",
      "Epoch 116/500\n",
      "2/2 - 0s - loss: 121.8515 - mse: 121.8515\n",
      "Epoch 117/500\n",
      "2/2 - 0s - loss: 125.6948 - mse: 125.6948\n",
      "Epoch 118/500\n",
      "2/2 - 0s - loss: 124.5279 - mse: 124.5279\n",
      "Epoch 119/500\n",
      "2/2 - 0s - loss: 119.1994 - mse: 119.1994\n",
      "Epoch 120/500\n",
      "2/2 - 0s - loss: 117.5370 - mse: 117.5370\n",
      "Epoch 121/500\n",
      "2/2 - 0s - loss: 119.8230 - mse: 119.8230\n",
      "Epoch 122/500\n",
      "2/2 - 0s - loss: 133.9023 - mse: 133.9023\n",
      "Epoch 123/500\n",
      "2/2 - 0s - loss: 116.8039 - mse: 116.8039\n",
      "Epoch 124/500\n",
      "2/2 - 0s - loss: 116.4773 - mse: 116.4773\n",
      "Epoch 125/500\n",
      "2/2 - 0s - loss: 115.3371 - mse: 115.3371\n",
      "Epoch 126/500\n",
      "2/2 - 0s - loss: 114.8796 - mse: 114.8796\n",
      "Epoch 127/500\n",
      "2/2 - 0s - loss: 113.6889 - mse: 113.6889\n",
      "Epoch 128/500\n",
      "2/2 - 0s - loss: 111.2685 - mse: 111.2685\n",
      "Epoch 129/500\n",
      "2/2 - 0s - loss: 110.2244 - mse: 110.2244\n",
      "Epoch 130/500\n",
      "2/2 - 0s - loss: 114.3055 - mse: 114.3055\n",
      "Epoch 131/500\n",
      "2/2 - 0s - loss: 115.7264 - mse: 115.7264\n",
      "Epoch 132/500\n",
      "2/2 - 0s - loss: 106.9706 - mse: 106.9706\n",
      "Epoch 133/500\n",
      "2/2 - 0s - loss: 108.5206 - mse: 108.5206\n",
      "Epoch 134/500\n",
      "2/2 - 0s - loss: 111.0975 - mse: 111.0975\n",
      "Epoch 135/500\n",
      "2/2 - 0s - loss: 119.8981 - mse: 119.8981\n",
      "Epoch 136/500\n",
      "2/2 - 0s - loss: 107.9277 - mse: 107.9277\n",
      "Epoch 137/500\n",
      "2/2 - 0s - loss: 103.8002 - mse: 103.8002\n",
      "Epoch 138/500\n",
      "2/2 - 0s - loss: 103.2383 - mse: 103.2383\n",
      "Epoch 139/500\n",
      "2/2 - 0s - loss: 104.1800 - mse: 104.1800\n",
      "Epoch 140/500\n",
      "2/2 - 0s - loss: 102.4059 - mse: 102.4059\n",
      "Epoch 141/500\n",
      "2/2 - 0s - loss: 101.4851 - mse: 101.4851\n",
      "Epoch 142/500\n",
      "2/2 - 0s - loss: 102.5869 - mse: 102.5869\n",
      "Epoch 143/500\n",
      "2/2 - 0s - loss: 102.5759 - mse: 102.5759\n",
      "Epoch 144/500\n",
      "2/2 - 0s - loss: 105.6947 - mse: 105.6947\n",
      "Epoch 145/500\n",
      "2/2 - 0s - loss: 99.3853 - mse: 99.3853\n",
      "Epoch 146/500\n",
      "2/2 - 0s - loss: 102.1696 - mse: 102.1696\n",
      "Epoch 147/500\n",
      "2/2 - 0s - loss: 97.5893 - mse: 97.5893\n",
      "Epoch 148/500\n",
      "2/2 - 0s - loss: 98.8673 - mse: 98.8673\n",
      "Epoch 149/500\n",
      "2/2 - 0s - loss: 104.9833 - mse: 104.9833\n",
      "Epoch 150/500\n",
      "2/2 - 0s - loss: 96.8110 - mse: 96.8110\n",
      "Epoch 151/500\n",
      "2/2 - 0s - loss: 93.8960 - mse: 93.8960\n",
      "Epoch 152/500\n",
      "2/2 - 0s - loss: 93.8899 - mse: 93.8899\n",
      "Epoch 153/500\n",
      "2/2 - 0s - loss: 93.7561 - mse: 93.7561\n",
      "Epoch 154/500\n",
      "2/2 - 0s - loss: 91.8623 - mse: 91.8623\n",
      "Epoch 155/500\n",
      "2/2 - 0s - loss: 89.9167 - mse: 89.9167\n",
      "Epoch 156/500\n",
      "2/2 - 0s - loss: 89.9027 - mse: 89.9027\n",
      "Epoch 157/500\n",
      "2/2 - 0s - loss: 109.6592 - mse: 109.6592\n",
      "Epoch 158/500\n",
      "2/2 - 0s - loss: 94.1149 - mse: 94.1149\n",
      "Epoch 159/500\n",
      "2/2 - 0s - loss: 86.9898 - mse: 86.9898\n",
      "Epoch 160/500\n",
      "2/2 - 0s - loss: 91.4203 - mse: 91.4203\n",
      "Epoch 161/500\n",
      "2/2 - 0s - loss: 90.9875 - mse: 90.9875\n",
      "Epoch 162/500\n",
      "2/2 - 0s - loss: 87.2190 - mse: 87.2190\n",
      "Epoch 163/500\n",
      "2/2 - 0s - loss: 91.8921 - mse: 91.8921\n",
      "Epoch 164/500\n",
      "2/2 - 0s - loss: 85.8350 - mse: 85.8350\n",
      "Epoch 165/500\n",
      "2/2 - 0s - loss: 84.7106 - mse: 84.7106\n",
      "Epoch 166/500\n",
      "2/2 - 0s - loss: 95.2861 - mse: 95.2861\n",
      "Epoch 167/500\n",
      "2/2 - 0s - loss: 86.4603 - mse: 86.4603\n",
      "Epoch 168/500\n",
      "2/2 - 0s - loss: 81.5853 - mse: 81.5853\n",
      "Epoch 169/500\n",
      "2/2 - 0s - loss: 81.1233 - mse: 81.1233\n",
      "Epoch 170/500\n",
      "2/2 - 0s - loss: 80.1791 - mse: 80.1791\n",
      "Epoch 171/500\n",
      "2/2 - 0s - loss: 81.3522 - mse: 81.3522\n",
      "Epoch 172/500\n",
      "2/2 - 0s - loss: 95.5564 - mse: 95.5564\n",
      "Epoch 173/500\n",
      "2/2 - 0s - loss: 79.9378 - mse: 79.9378\n",
      "Epoch 174/500\n",
      "2/2 - 0s - loss: 93.6978 - mse: 93.6978\n",
      "Epoch 175/500\n",
      "2/2 - 0s - loss: 86.0135 - mse: 86.0135\n",
      "Epoch 176/500\n",
      "2/2 - 0s - loss: 77.3745 - mse: 77.3745\n",
      "Epoch 177/500\n",
      "2/2 - 0s - loss: 77.3980 - mse: 77.3980\n",
      "Epoch 178/500\n",
      "2/2 - 0s - loss: 80.7782 - mse: 80.7782\n",
      "Epoch 179/500\n",
      "2/2 - 0s - loss: 87.5617 - mse: 87.5617\n",
      "Epoch 180/500\n",
      "2/2 - 0s - loss: 78.2775 - mse: 78.2775\n",
      "Epoch 181/500\n",
      "2/2 - 0s - loss: 75.0401 - mse: 75.0401\n",
      "Epoch 182/500\n",
      "2/2 - 0s - loss: 75.2838 - mse: 75.2838\n",
      "Epoch 183/500\n",
      "2/2 - 0s - loss: 73.1324 - mse: 73.1324\n",
      "Epoch 184/500\n",
      "2/2 - 0s - loss: 72.6074 - mse: 72.6074\n",
      "Epoch 185/500\n",
      "2/2 - 0s - loss: 77.1006 - mse: 77.1006\n",
      "Epoch 186/500\n",
      "2/2 - 0s - loss: 82.5525 - mse: 82.5525\n",
      "Epoch 187/500\n",
      "2/2 - 0s - loss: 82.1321 - mse: 82.1321\n",
      "Epoch 188/500\n",
      "2/2 - 0s - loss: 73.9168 - mse: 73.9168\n",
      "Epoch 189/500\n",
      "2/2 - 0s - loss: 76.9834 - mse: 76.9834\n",
      "Epoch 190/500\n",
      "2/2 - 0s - loss: 70.3984 - mse: 70.3984\n",
      "Epoch 191/500\n",
      "2/2 - 0s - loss: 69.4548 - mse: 69.4548\n",
      "Epoch 192/500\n",
      "2/2 - 0s - loss: 69.4201 - mse: 69.4201\n",
      "Epoch 193/500\n",
      "2/2 - 0s - loss: 76.2563 - mse: 76.2563\n",
      "Epoch 194/500\n",
      "2/2 - 0s - loss: 79.4705 - mse: 79.4705\n",
      "Epoch 195/500\n",
      "2/2 - 0s - loss: 69.0070 - mse: 69.0070\n",
      "Epoch 196/500\n",
      "2/2 - 0s - loss: 68.9077 - mse: 68.9077\n",
      "Epoch 197/500\n",
      "2/2 - 0s - loss: 68.2669 - mse: 68.2669\n",
      "Epoch 198/500\n",
      "2/2 - 0s - loss: 65.8174 - mse: 65.8174\n",
      "Epoch 199/500\n",
      "2/2 - 0s - loss: 68.1393 - mse: 68.1393\n",
      "Epoch 200/500\n",
      "2/2 - 0s - loss: 64.4355 - mse: 64.4355\n",
      "Epoch 201/500\n",
      "2/2 - 0s - loss: 65.1890 - mse: 65.1890\n",
      "Epoch 202/500\n",
      "2/2 - 0s - loss: 62.9781 - mse: 62.9781\n",
      "Epoch 203/500\n",
      "2/2 - 0s - loss: 62.6510 - mse: 62.6510\n",
      "Epoch 204/500\n",
      "2/2 - 0s - loss: 66.0340 - mse: 66.0340\n",
      "Epoch 205/500\n",
      "2/2 - 0s - loss: 62.4263 - mse: 62.4263\n",
      "Epoch 206/500\n",
      "2/2 - 0s - loss: 66.0832 - mse: 66.0832\n",
      "Epoch 207/500\n",
      "2/2 - 0s - loss: 60.2942 - mse: 60.2942\n",
      "Epoch 208/500\n",
      "2/2 - 0s - loss: 61.5384 - mse: 61.5384\n",
      "Epoch 209/500\n",
      "2/2 - 0s - loss: 65.0545 - mse: 65.0545\n",
      "Epoch 210/500\n",
      "2/2 - 0s - loss: 64.5653 - mse: 64.5653\n",
      "Epoch 211/500\n",
      "2/2 - 0s - loss: 60.8980 - mse: 60.8980\n",
      "Epoch 212/500\n",
      "2/2 - 0s - loss: 77.2318 - mse: 77.2318\n",
      "Epoch 213/500\n",
      "2/2 - 0s - loss: 60.5017 - mse: 60.5017\n",
      "Epoch 214/500\n",
      "2/2 - 0s - loss: 56.7349 - mse: 56.7349\n",
      "Epoch 215/500\n",
      "2/2 - 0s - loss: 55.7287 - mse: 55.7287\n",
      "Epoch 216/500\n",
      "2/2 - 0s - loss: 55.4489 - mse: 55.4489\n",
      "Epoch 217/500\n",
      "2/2 - 0s - loss: 57.3140 - mse: 57.3140\n",
      "Epoch 218/500\n",
      "2/2 - 0s - loss: 56.1721 - mse: 56.1721\n",
      "Epoch 219/500\n",
      "2/2 - 0s - loss: 55.3760 - mse: 55.3760\n",
      "Epoch 220/500\n",
      "2/2 - 0s - loss: 56.6162 - mse: 56.6162\n",
      "Epoch 221/500\n",
      "2/2 - 0s - loss: 55.7626 - mse: 55.7626\n",
      "Epoch 222/500\n",
      "2/2 - 0s - loss: 61.6402 - mse: 61.6402\n",
      "Epoch 223/500\n",
      "2/2 - 0s - loss: 54.1392 - mse: 54.1392\n",
      "Epoch 224/500\n",
      "2/2 - 0s - loss: 54.0870 - mse: 54.0870\n",
      "Epoch 225/500\n",
      "2/2 - 0s - loss: 55.5776 - mse: 55.5776\n",
      "Epoch 226/500\n",
      "2/2 - 0s - loss: 59.3838 - mse: 59.3838\n",
      "Epoch 227/500\n",
      "2/2 - 0s - loss: 61.3159 - mse: 61.3159\n",
      "Epoch 228/500\n",
      "2/2 - 0s - loss: 51.0937 - mse: 51.0937\n",
      "Epoch 229/500\n",
      "2/2 - 0s - loss: 50.2519 - mse: 50.2519\n",
      "Epoch 230/500\n",
      "2/2 - 0s - loss: 50.9126 - mse: 50.9126\n",
      "Epoch 231/500\n",
      "2/2 - 0s - loss: 49.2193 - mse: 49.2193\n",
      "Epoch 232/500\n",
      "2/2 - 0s - loss: 49.7654 - mse: 49.7654\n",
      "Epoch 233/500\n",
      "2/2 - 0s - loss: 47.9905 - mse: 47.9905\n",
      "Epoch 234/500\n",
      "2/2 - 0s - loss: 50.7049 - mse: 50.7049\n",
      "Epoch 235/500\n",
      "2/2 - 0s - loss: 60.7071 - mse: 60.7071\n",
      "Epoch 236/500\n",
      "2/2 - 0s - loss: 52.8667 - mse: 52.8667\n",
      "Epoch 237/500\n",
      "2/2 - 0s - loss: 49.1877 - mse: 49.1877\n",
      "Epoch 238/500\n",
      "2/2 - 0s - loss: 46.3979 - mse: 46.3979\n",
      "Epoch 239/500\n",
      "2/2 - 0s - loss: 47.3980 - mse: 47.3980\n",
      "Epoch 240/500\n",
      "2/2 - 0s - loss: 45.2751 - mse: 45.2751\n",
      "Epoch 241/500\n",
      "2/2 - 0s - loss: 45.1038 - mse: 45.1038\n",
      "Epoch 242/500\n",
      "2/2 - 0s - loss: 44.5416 - mse: 44.5416\n",
      "Epoch 243/500\n",
      "2/2 - 0s - loss: 49.0550 - mse: 49.0550\n",
      "Epoch 244/500\n",
      "2/2 - 0s - loss: 43.5911 - mse: 43.5911\n",
      "Epoch 245/500\n",
      "2/2 - 0s - loss: 46.0900 - mse: 46.0900\n",
      "Epoch 246/500\n",
      "2/2 - 0s - loss: 54.3643 - mse: 54.3643\n",
      "Epoch 247/500\n",
      "2/2 - 0s - loss: 43.1470 - mse: 43.1470\n",
      "Epoch 248/500\n",
      "2/2 - 0s - loss: 47.6150 - mse: 47.6150\n",
      "Epoch 249/500\n",
      "2/2 - 0s - loss: 43.4502 - mse: 43.4502\n",
      "Epoch 250/500\n",
      "2/2 - 0s - loss: 44.2961 - mse: 44.2961\n",
      "Epoch 251/500\n",
      "2/2 - 0s - loss: 42.2177 - mse: 42.2177\n",
      "Epoch 252/500\n",
      "2/2 - 0s - loss: 40.9660 - mse: 40.9660\n",
      "Epoch 253/500\n",
      "2/2 - 0s - loss: 47.6545 - mse: 47.6545\n",
      "Epoch 254/500\n",
      "2/2 - 0s - loss: 43.2142 - mse: 43.2142\n",
      "Epoch 255/500\n",
      "2/2 - 0s - loss: 50.0550 - mse: 50.0550\n",
      "Epoch 256/500\n",
      "2/2 - 0s - loss: 41.5904 - mse: 41.5904\n",
      "Epoch 257/500\n",
      "2/2 - 0s - loss: 39.1612 - mse: 39.1612\n",
      "Epoch 258/500\n",
      "2/2 - 0s - loss: 43.9686 - mse: 43.9686\n",
      "Epoch 259/500\n",
      "2/2 - 0s - loss: 41.9055 - mse: 41.9055\n",
      "Epoch 260/500\n",
      "2/2 - 0s - loss: 41.0061 - mse: 41.0061\n",
      "Epoch 261/500\n",
      "2/2 - 0s - loss: 39.4422 - mse: 39.4422\n",
      "Epoch 262/500\n",
      "2/2 - 0s - loss: 38.4836 - mse: 38.4836\n",
      "Epoch 263/500\n",
      "2/2 - 0s - loss: 36.6852 - mse: 36.6852\n",
      "Epoch 264/500\n",
      "2/2 - 0s - loss: 36.8994 - mse: 36.8994\n",
      "Epoch 265/500\n",
      "2/2 - 0s - loss: 37.4679 - mse: 37.4679\n",
      "Epoch 266/500\n",
      "2/2 - 0s - loss: 44.6177 - mse: 44.6177\n",
      "Epoch 267/500\n",
      "2/2 - 0s - loss: 48.7540 - mse: 48.7540\n",
      "Epoch 268/500\n",
      "2/2 - 0s - loss: 37.6068 - mse: 37.6068\n",
      "Epoch 269/500\n",
      "2/2 - 0s - loss: 35.5877 - mse: 35.5877\n",
      "Epoch 270/500\n",
      "2/2 - 0s - loss: 34.4534 - mse: 34.4534\n",
      "Epoch 271/500\n",
      "2/2 - 0s - loss: 34.5439 - mse: 34.5439\n",
      "Epoch 272/500\n",
      "2/2 - 0s - loss: 36.2211 - mse: 36.2211\n",
      "Epoch 273/500\n",
      "2/2 - 0s - loss: 33.6033 - mse: 33.6033\n",
      "Epoch 274/500\n",
      "2/2 - 0s - loss: 45.9516 - mse: 45.9516\n",
      "Epoch 275/500\n",
      "2/2 - 0s - loss: 37.7969 - mse: 37.7969\n",
      "Epoch 276/500\n",
      "2/2 - 0s - loss: 33.8418 - mse: 33.8418\n",
      "Epoch 277/500\n",
      "2/2 - 0s - loss: 37.2417 - mse: 37.2417\n",
      "Epoch 278/500\n",
      "2/2 - 0s - loss: 36.4384 - mse: 36.4384\n",
      "Epoch 279/500\n",
      "2/2 - 0s - loss: 33.5616 - mse: 33.5616\n",
      "Epoch 280/500\n",
      "2/2 - 0s - loss: 34.9187 - mse: 34.9187\n",
      "Epoch 281/500\n",
      "2/2 - 0s - loss: 37.3018 - mse: 37.3018\n",
      "Epoch 282/500\n",
      "2/2 - 0s - loss: 35.8408 - mse: 35.8408\n",
      "Epoch 283/500\n",
      "2/2 - 0s - loss: 31.2712 - mse: 31.2712\n",
      "Epoch 284/500\n",
      "2/2 - 0s - loss: 32.0938 - mse: 32.0938\n",
      "Epoch 285/500\n",
      "2/2 - 0s - loss: 30.8676 - mse: 30.8676\n",
      "Epoch 286/500\n",
      "2/2 - 0s - loss: 30.9054 - mse: 30.9054\n",
      "Epoch 287/500\n",
      "2/2 - 0s - loss: 32.7639 - mse: 32.7639\n",
      "Epoch 288/500\n",
      "2/2 - 0s - loss: 30.3808 - mse: 30.3808\n",
      "Epoch 289/500\n",
      "2/2 - 0s - loss: 29.1711 - mse: 29.1711\n",
      "Epoch 290/500\n",
      "2/2 - 0s - loss: 32.0853 - mse: 32.0853\n",
      "Epoch 291/500\n",
      "2/2 - 0s - loss: 29.8701 - mse: 29.8701\n",
      "Epoch 292/500\n",
      "2/2 - 0s - loss: 29.7538 - mse: 29.7538\n",
      "Epoch 293/500\n",
      "2/2 - 0s - loss: 29.9357 - mse: 29.9357\n",
      "Epoch 294/500\n",
      "2/2 - 0s - loss: 34.2084 - mse: 34.2084\n",
      "Epoch 295/500\n",
      "2/2 - 0s - loss: 34.2392 - mse: 34.2392\n",
      "Epoch 296/500\n",
      "2/2 - 0s - loss: 28.4251 - mse: 28.4251\n",
      "Epoch 297/500\n",
      "2/2 - 0s - loss: 27.0648 - mse: 27.0648\n",
      "Epoch 298/500\n",
      "2/2 - 0s - loss: 29.8011 - mse: 29.8011\n",
      "Epoch 299/500\n",
      "2/2 - 0s - loss: 29.5839 - mse: 29.5839\n",
      "Epoch 300/500\n",
      "2/2 - 0s - loss: 26.8772 - mse: 26.8772\n",
      "Epoch 301/500\n",
      "2/2 - 0s - loss: 35.2980 - mse: 35.2980\n",
      "Epoch 302/500\n",
      "2/2 - 0s - loss: 33.0344 - mse: 33.0344\n",
      "Epoch 303/500\n",
      "2/2 - 0s - loss: 27.2365 - mse: 27.2365\n",
      "Epoch 304/500\n",
      "2/2 - 0s - loss: 37.9097 - mse: 37.9097\n",
      "Epoch 305/500\n",
      "2/2 - 0s - loss: 29.5057 - mse: 29.5057\n",
      "Epoch 306/500\n",
      "2/2 - 0s - loss: 25.3314 - mse: 25.3314\n",
      "Epoch 307/500\n",
      "2/2 - 0s - loss: 26.4672 - mse: 26.4672\n",
      "Epoch 308/500\n",
      "2/2 - 0s - loss: 25.1123 - mse: 25.1123\n",
      "Epoch 309/500\n",
      "2/2 - 0s - loss: 24.9388 - mse: 24.9388\n",
      "Epoch 310/500\n",
      "2/2 - 0s - loss: 25.3458 - mse: 25.3458\n",
      "Epoch 311/500\n",
      "2/2 - 0s - loss: 27.1251 - mse: 27.1251\n",
      "Epoch 312/500\n",
      "2/2 - 0s - loss: 32.3804 - mse: 32.3804\n",
      "Epoch 313/500\n",
      "2/2 - 0s - loss: 27.3462 - mse: 27.3462\n",
      "Epoch 314/500\n",
      "2/2 - 0s - loss: 25.4674 - mse: 25.4674\n",
      "Epoch 315/500\n",
      "2/2 - 0s - loss: 23.6041 - mse: 23.6041\n",
      "Epoch 316/500\n",
      "2/2 - 0s - loss: 23.2043 - mse: 23.2043\n",
      "Epoch 317/500\n",
      "2/2 - 0s - loss: 26.5859 - mse: 26.5859\n",
      "Epoch 318/500\n",
      "2/2 - 0s - loss: 25.9936 - mse: 25.9936\n",
      "Epoch 319/500\n",
      "2/2 - 0s - loss: 24.9759 - mse: 24.9759\n",
      "Epoch 320/500\n",
      "2/2 - 0s - loss: 31.2731 - mse: 31.2731\n",
      "Epoch 321/500\n",
      "2/2 - 0s - loss: 31.7667 - mse: 31.7667\n",
      "Epoch 322/500\n",
      "2/2 - 0s - loss: 22.9771 - mse: 22.9771\n",
      "Epoch 323/500\n",
      "2/2 - 0s - loss: 24.3856 - mse: 24.3856\n",
      "Epoch 324/500\n",
      "2/2 - 0s - loss: 23.4784 - mse: 23.4784\n",
      "Epoch 325/500\n",
      "2/2 - 0s - loss: 22.2336 - mse: 22.2336\n",
      "Epoch 326/500\n",
      "2/2 - 0s - loss: 28.1425 - mse: 28.1425\n",
      "Epoch 327/500\n",
      "2/2 - 0s - loss: 25.9876 - mse: 25.9876\n",
      "Epoch 328/500\n",
      "2/2 - 0s - loss: 24.0260 - mse: 24.0260\n",
      "Epoch 329/500\n",
      "2/2 - 0s - loss: 23.9695 - mse: 23.9695\n",
      "Epoch 330/500\n",
      "2/2 - 0s - loss: 21.3460 - mse: 21.3460\n",
      "Epoch 331/500\n",
      "2/2 - 0s - loss: 22.6967 - mse: 22.6967\n",
      "Epoch 332/500\n",
      "2/2 - 0s - loss: 20.5568 - mse: 20.5568\n",
      "Epoch 333/500\n",
      "2/2 - 0s - loss: 26.3434 - mse: 26.3434\n",
      "Epoch 334/500\n",
      "2/2 - 0s - loss: 22.3101 - mse: 22.3101\n",
      "Epoch 335/500\n",
      "2/2 - 0s - loss: 21.1823 - mse: 21.1823\n",
      "Epoch 336/500\n",
      "2/2 - 0s - loss: 21.7780 - mse: 21.7780\n",
      "Epoch 337/500\n",
      "2/2 - 0s - loss: 25.5234 - mse: 25.5234\n",
      "Epoch 338/500\n",
      "2/2 - 0s - loss: 24.8171 - mse: 24.8171\n",
      "Epoch 339/500\n",
      "2/2 - 0s - loss: 19.4961 - mse: 19.4961\n",
      "Epoch 340/500\n",
      "2/2 - 0s - loss: 19.2088 - mse: 19.2088\n",
      "Epoch 341/500\n",
      "2/2 - 0s - loss: 21.7394 - mse: 21.7394\n",
      "Epoch 342/500\n",
      "2/2 - 0s - loss: 19.2023 - mse: 19.2023\n",
      "Epoch 343/500\n",
      "2/2 - 0s - loss: 27.5906 - mse: 27.5906\n",
      "Epoch 344/500\n",
      "2/2 - 0s - loss: 23.0415 - mse: 23.0415\n",
      "Epoch 345/500\n",
      "2/2 - 0s - loss: 23.6262 - mse: 23.6262\n",
      "Epoch 346/500\n",
      "2/2 - 0s - loss: 18.6377 - mse: 18.6377\n",
      "Epoch 347/500\n",
      "2/2 - 0s - loss: 18.5990 - mse: 18.5990\n",
      "Epoch 348/500\n",
      "2/2 - 0s - loss: 19.5520 - mse: 19.5520\n",
      "Epoch 349/500\n",
      "2/2 - 0s - loss: 20.6642 - mse: 20.6642\n",
      "Epoch 350/500\n",
      "2/2 - 0s - loss: 23.9259 - mse: 23.9259\n",
      "Epoch 351/500\n",
      "2/2 - 0s - loss: 17.8709 - mse: 17.8709\n",
      "Epoch 352/500\n",
      "2/2 - 0s - loss: 17.5203 - mse: 17.5203\n",
      "Epoch 353/500\n",
      "2/2 - 0s - loss: 20.9335 - mse: 20.9335\n",
      "Epoch 354/500\n",
      "2/2 - 0s - loss: 19.8532 - mse: 19.8532\n",
      "Epoch 355/500\n",
      "2/2 - 0s - loss: 17.9333 - mse: 17.9333\n",
      "Epoch 356/500\n",
      "2/2 - 0s - loss: 20.9586 - mse: 20.9586\n",
      "Epoch 357/500\n",
      "2/2 - 0s - loss: 17.2535 - mse: 17.2535\n",
      "Epoch 358/500\n",
      "2/2 - 0s - loss: 17.6886 - mse: 17.6886\n",
      "Epoch 359/500\n",
      "2/2 - 0s - loss: 25.0085 - mse: 25.0085\n",
      "Epoch 360/500\n",
      "2/2 - 0s - loss: 20.9869 - mse: 20.9869\n",
      "Epoch 361/500\n",
      "2/2 - 0s - loss: 24.3833 - mse: 24.3833\n",
      "Epoch 362/500\n",
      "2/2 - 0s - loss: 24.2657 - mse: 24.2657\n",
      "Epoch 363/500\n",
      "2/2 - 0s - loss: 18.6480 - mse: 18.6480\n",
      "Epoch 364/500\n",
      "2/2 - 0s - loss: 15.8987 - mse: 15.8987\n",
      "Epoch 365/500\n",
      "2/2 - 0s - loss: 17.0063 - mse: 17.0063\n",
      "Epoch 366/500\n",
      "2/2 - 0s - loss: 21.8362 - mse: 21.8362\n",
      "Epoch 367/500\n",
      "2/2 - 0s - loss: 17.0793 - mse: 17.0793\n",
      "Epoch 368/500\n",
      "2/2 - 0s - loss: 17.7531 - mse: 17.7531\n",
      "Epoch 369/500\n",
      "2/2 - 0s - loss: 15.6082 - mse: 15.6082\n",
      "Epoch 370/500\n",
      "2/2 - 0s - loss: 15.7292 - mse: 15.7292\n",
      "Epoch 371/500\n",
      "2/2 - 0s - loss: 15.1184 - mse: 15.1184\n",
      "Epoch 372/500\n",
      "2/2 - 0s - loss: 15.4045 - mse: 15.4045\n",
      "Epoch 373/500\n",
      "2/2 - 0s - loss: 14.9053 - mse: 14.9053\n",
      "Epoch 374/500\n",
      "2/2 - 0s - loss: 17.8509 - mse: 17.8509\n",
      "Epoch 375/500\n",
      "2/2 - 0s - loss: 19.1974 - mse: 19.1974\n",
      "Epoch 376/500\n",
      "2/2 - 0s - loss: 16.3427 - mse: 16.3427\n",
      "Epoch 377/500\n",
      "2/2 - 0s - loss: 15.3541 - mse: 15.3541\n",
      "Epoch 378/500\n",
      "2/2 - 0s - loss: 14.3178 - mse: 14.3178\n",
      "Epoch 379/500\n",
      "2/2 - 0s - loss: 14.1869 - mse: 14.1869\n",
      "Epoch 380/500\n",
      "2/2 - 0s - loss: 13.9054 - mse: 13.9054\n",
      "Epoch 381/500\n",
      "2/2 - 0s - loss: 13.7115 - mse: 13.7115\n",
      "Epoch 382/500\n",
      "2/2 - 0s - loss: 15.7427 - mse: 15.7427\n",
      "Epoch 383/500\n",
      "2/2 - 0s - loss: 32.3157 - mse: 32.3157\n",
      "Epoch 384/500\n",
      "2/2 - 0s - loss: 14.4398 - mse: 14.4398\n",
      "Epoch 385/500\n",
      "2/2 - 0s - loss: 15.8419 - mse: 15.8419\n",
      "Epoch 386/500\n",
      "2/2 - 0s - loss: 13.8117 - mse: 13.8117\n",
      "Epoch 387/500\n",
      "2/2 - 0s - loss: 14.0211 - mse: 14.0211\n",
      "Epoch 388/500\n",
      "2/2 - 0s - loss: 13.1600 - mse: 13.1600\n",
      "Epoch 389/500\n",
      "2/2 - 0s - loss: 13.7124 - mse: 13.7124\n",
      "Epoch 390/500\n",
      "2/2 - 0s - loss: 15.6518 - mse: 15.6518\n",
      "Epoch 391/500\n",
      "2/2 - 0s - loss: 17.3632 - mse: 17.3632\n",
      "Epoch 392/500\n",
      "2/2 - 0s - loss: 12.7208 - mse: 12.7208\n",
      "Epoch 393/500\n",
      "2/2 - 0s - loss: 14.3446 - mse: 14.3446\n",
      "Epoch 394/500\n",
      "2/2 - 0s - loss: 14.7310 - mse: 14.7310\n",
      "Epoch 395/500\n",
      "2/2 - 0s - loss: 15.3891 - mse: 15.3891\n",
      "Epoch 396/500\n",
      "2/2 - 0s - loss: 13.5271 - mse: 13.5271\n",
      "Epoch 397/500\n",
      "2/2 - 0s - loss: 12.5184 - mse: 12.5184\n",
      "Epoch 398/500\n",
      "2/2 - 0s - loss: 13.5046 - mse: 13.5046\n",
      "Epoch 399/500\n",
      "2/2 - 0s - loss: 12.9775 - mse: 12.9775\n",
      "Epoch 400/500\n",
      "2/2 - 0s - loss: 12.1622 - mse: 12.1622\n",
      "Epoch 401/500\n",
      "2/2 - 0s - loss: 14.9847 - mse: 14.9847\n",
      "Epoch 402/500\n",
      "2/2 - 0s - loss: 24.4113 - mse: 24.4113\n",
      "Epoch 403/500\n",
      "2/2 - 0s - loss: 12.6252 - mse: 12.6252\n",
      "Epoch 404/500\n",
      "2/2 - 0s - loss: 14.5304 - mse: 14.5304\n",
      "Epoch 405/500\n",
      "2/2 - 0s - loss: 14.6311 - mse: 14.6311\n",
      "Epoch 406/500\n",
      "2/2 - 0s - loss: 15.4084 - mse: 15.4084\n",
      "Epoch 407/500\n",
      "2/2 - 0s - loss: 15.0132 - mse: 15.0132\n",
      "Epoch 408/500\n",
      "2/2 - 0s - loss: 18.1020 - mse: 18.1020\n",
      "Epoch 409/500\n",
      "2/2 - 0s - loss: 11.8113 - mse: 11.8113\n",
      "Epoch 410/500\n",
      "2/2 - 0s - loss: 13.0375 - mse: 13.0375\n",
      "Epoch 411/500\n",
      "2/2 - 0s - loss: 11.5725 - mse: 11.5725\n",
      "Epoch 412/500\n",
      "2/2 - 0s - loss: 11.3843 - mse: 11.3843\n",
      "Epoch 413/500\n",
      "2/2 - 0s - loss: 12.8532 - mse: 12.8532\n",
      "Epoch 414/500\n",
      "2/2 - 0s - loss: 23.9491 - mse: 23.9491\n",
      "Epoch 415/500\n",
      "2/2 - 0s - loss: 19.0092 - mse: 19.0092\n",
      "Epoch 416/500\n",
      "2/2 - 0s - loss: 11.7310 - mse: 11.7310\n",
      "Epoch 417/500\n",
      "2/2 - 0s - loss: 10.9387 - mse: 10.9387\n",
      "Epoch 418/500\n",
      "2/2 - 0s - loss: 10.8151 - mse: 10.8151\n",
      "Epoch 419/500\n",
      "2/2 - 0s - loss: 10.7425 - mse: 10.7425\n",
      "Epoch 420/500\n",
      "2/2 - 0s - loss: 10.6624 - mse: 10.6624\n",
      "Epoch 421/500\n",
      "2/2 - 0s - loss: 11.0356 - mse: 11.0356\n",
      "Epoch 422/500\n",
      "2/2 - 0s - loss: 10.5118 - mse: 10.5118\n",
      "Epoch 423/500\n",
      "2/2 - 0s - loss: 14.2933 - mse: 14.2933\n",
      "Epoch 424/500\n",
      "2/2 - 0s - loss: 14.1762 - mse: 14.1762\n",
      "Epoch 425/500\n",
      "2/2 - 0s - loss: 11.3829 - mse: 11.3829\n",
      "Epoch 426/500\n",
      "2/2 - 0s - loss: 15.3044 - mse: 15.3044\n",
      "Epoch 427/500\n",
      "2/2 - 0s - loss: 12.7110 - mse: 12.7110\n",
      "Epoch 428/500\n",
      "2/2 - 0s - loss: 11.8328 - mse: 11.8328\n",
      "Epoch 429/500\n",
      "2/2 - 0s - loss: 12.5070 - mse: 12.5070\n",
      "Epoch 430/500\n",
      "2/2 - 0s - loss: 14.7674 - mse: 14.7674\n",
      "Epoch 431/500\n",
      "2/2 - 0s - loss: 12.9814 - mse: 12.9814\n",
      "Epoch 432/500\n",
      "2/2 - 0s - loss: 12.1712 - mse: 12.1712\n",
      "Epoch 433/500\n",
      "2/2 - 0s - loss: 11.8233 - mse: 11.8233\n",
      "Epoch 434/500\n",
      "2/2 - 0s - loss: 13.6047 - mse: 13.6047\n",
      "Epoch 435/500\n",
      "2/2 - 0s - loss: 11.7979 - mse: 11.7979\n",
      "Epoch 436/500\n",
      "2/2 - 0s - loss: 9.9654 - mse: 9.9654\n",
      "Epoch 437/500\n",
      "2/2 - 0s - loss: 12.1306 - mse: 12.1306\n",
      "Epoch 438/500\n",
      "2/2 - 0s - loss: 15.5417 - mse: 15.5417\n",
      "Epoch 439/500\n",
      "2/2 - 0s - loss: 11.3063 - mse: 11.3063\n",
      "Epoch 440/500\n",
      "2/2 - 0s - loss: 9.9276 - mse: 9.9276\n",
      "Epoch 441/500\n",
      "2/2 - 0s - loss: 14.3163 - mse: 14.3163\n",
      "Epoch 442/500\n",
      "2/2 - 0s - loss: 10.4338 - mse: 10.4338\n",
      "Epoch 443/500\n",
      "2/2 - 0s - loss: 12.0623 - mse: 12.0623\n",
      "Epoch 444/500\n",
      "2/2 - 0s - loss: 15.3261 - mse: 15.3261\n",
      "Epoch 445/500\n",
      "2/2 - 0s - loss: 10.8702 - mse: 10.8702\n",
      "Epoch 446/500\n",
      "2/2 - 0s - loss: 11.0242 - mse: 11.0242\n",
      "Epoch 447/500\n",
      "2/2 - 0s - loss: 10.5958 - mse: 10.5958\n",
      "Epoch 448/500\n",
      "2/2 - 0s - loss: 12.3817 - mse: 12.3817\n",
      "Epoch 449/500\n",
      "2/2 - 0s - loss: 12.8805 - mse: 12.8805\n",
      "Epoch 450/500\n",
      "2/2 - 0s - loss: 9.1898 - mse: 9.1898\n",
      "Epoch 451/500\n",
      "2/2 - 0s - loss: 9.2808 - mse: 9.2808\n",
      "Epoch 452/500\n",
      "2/2 - 0s - loss: 9.9828 - mse: 9.9828\n",
      "Epoch 453/500\n",
      "2/2 - 0s - loss: 9.8542 - mse: 9.8542\n",
      "Epoch 454/500\n",
      "2/2 - 0s - loss: 8.9486 - mse: 8.9486\n",
      "Epoch 455/500\n",
      "2/2 - 0s - loss: 10.0516 - mse: 10.0516\n",
      "Epoch 456/500\n",
      "2/2 - 0s - loss: 12.2567 - mse: 12.2567\n",
      "Epoch 457/500\n",
      "2/2 - 0s - loss: 11.3415 - mse: 11.3415\n",
      "Epoch 458/500\n",
      "2/2 - 0s - loss: 10.4853 - mse: 10.4853\n",
      "Epoch 459/500\n",
      "2/2 - 0s - loss: 11.9789 - mse: 11.9789\n",
      "Epoch 460/500\n",
      "2/2 - 0s - loss: 20.2054 - mse: 20.2054\n",
      "Epoch 461/500\n",
      "2/2 - 0s - loss: 10.4916 - mse: 10.4916\n",
      "Epoch 462/500\n",
      "2/2 - 0s - loss: 9.6547 - mse: 9.6547\n",
      "Epoch 463/500\n",
      "2/2 - 0s - loss: 11.2976 - mse: 11.2976\n",
      "Epoch 464/500\n",
      "2/2 - 0s - loss: 10.3140 - mse: 10.3140\n",
      "Epoch 465/500\n",
      "2/2 - 0s - loss: 10.2353 - mse: 10.2353\n",
      "Epoch 466/500\n",
      "2/2 - 0s - loss: 17.3693 - mse: 17.3693\n",
      "Epoch 467/500\n",
      "2/2 - 0s - loss: 11.9730 - mse: 11.9730\n",
      "Epoch 468/500\n",
      "2/2 - 0s - loss: 9.9841 - mse: 9.9841\n",
      "Epoch 469/500\n",
      "2/2 - 0s - loss: 9.8347 - mse: 9.8347\n",
      "Epoch 470/500\n",
      "2/2 - 0s - loss: 9.1527 - mse: 9.1527\n",
      "Epoch 471/500\n",
      "2/2 - 0s - loss: 8.8800 - mse: 8.8800\n",
      "Epoch 472/500\n",
      "2/2 - 0s - loss: 9.8370 - mse: 9.8370\n",
      "Epoch 473/500\n",
      "2/2 - 0s - loss: 8.5683 - mse: 8.5683\n",
      "Epoch 474/500\n",
      "2/2 - 0s - loss: 15.5020 - mse: 15.5020\n",
      "Epoch 475/500\n",
      "2/2 - 0s - loss: 11.1752 - mse: 11.1752\n",
      "Epoch 476/500\n",
      "2/2 - 0s - loss: 8.7653 - mse: 8.7653\n",
      "Epoch 477/500\n",
      "2/2 - 0s - loss: 11.2804 - mse: 11.2804\n",
      "Epoch 478/500\n",
      "2/2 - 0s - loss: 8.9022 - mse: 8.9022\n",
      "Epoch 479/500\n",
      "2/2 - 0s - loss: 8.1697 - mse: 8.1697\n",
      "Epoch 480/500\n",
      "2/2 - 0s - loss: 8.1340 - mse: 8.1340\n",
      "Epoch 481/500\n",
      "2/2 - 0s - loss: 9.2465 - mse: 9.2465\n",
      "Epoch 482/500\n",
      "2/2 - 0s - loss: 15.4254 - mse: 15.4254\n",
      "Epoch 483/500\n",
      "2/2 - 0s - loss: 13.6191 - mse: 13.6191\n",
      "Epoch 484/500\n",
      "2/2 - 0s - loss: 12.1409 - mse: 12.1409\n",
      "Epoch 485/500\n",
      "2/2 - 0s - loss: 8.2620 - mse: 8.2620\n",
      "Epoch 486/500\n",
      "2/2 - 0s - loss: 10.4361 - mse: 10.4361\n",
      "Epoch 487/500\n",
      "2/2 - 0s - loss: 9.4482 - mse: 9.4482\n",
      "Epoch 488/500\n",
      "2/2 - 0s - loss: 10.7756 - mse: 10.7756\n",
      "Epoch 489/500\n",
      "2/2 - 0s - loss: 11.8693 - mse: 11.8693\n",
      "Epoch 490/500\n",
      "2/2 - 0s - loss: 9.3406 - mse: 9.3406\n",
      "Epoch 491/500\n",
      "2/2 - 0s - loss: 8.0052 - mse: 8.0052\n",
      "Epoch 492/500\n",
      "2/2 - 0s - loss: 7.7470 - mse: 7.7470\n",
      "Epoch 493/500\n",
      "2/2 - 0s - loss: 8.4638 - mse: 8.4638\n",
      "Epoch 494/500\n",
      "2/2 - 0s - loss: 7.7388 - mse: 7.7388\n",
      "Epoch 495/500\n",
      "2/2 - 0s - loss: 7.6394 - mse: 7.6394\n",
      "Epoch 496/500\n",
      "2/2 - 0s - loss: 7.6288 - mse: 7.6288\n",
      "Epoch 497/500\n",
      "2/2 - 0s - loss: 8.2863 - mse: 8.2863\n",
      "Epoch 498/500\n",
      "2/2 - 0s - loss: 18.4408 - mse: 18.4408\n",
      "Epoch 499/500\n",
      "2/2 - 0s - loss: 17.5369 - mse: 17.5369\n",
      "Epoch 500/500\n",
      "2/2 - 0s - loss: 13.3834 - mse: 13.3834\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x15f2d34c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:07:00.350414: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>22.246986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>18.858179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.835787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>18.440342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>16.650234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8             22.246986\n",
       "AK       18.1             18.858179\n",
       "AZ       18.6             16.835787\n",
       "AR       22.4             18.440342\n",
       "CA       12.0             16.650234"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en resumidas cuentas, ser√≠a todo as√≠:\n",
    "\n",
    "# 1) librerias\n",
    "from keras.models import Sequential # para crear el modelo secuencial\n",
    "from keras.layers import Dense, Input # para colocar el dense y el input de RRNN\n",
    "\n",
    "# 2) Definir el modelo\n",
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,))) # Es 6 pq hay 6 variables explicativas en el dataset\n",
    "# (el total no se cuenta porque es precisamente la variable objetiva, la que buscamos)\n",
    "model.add(layer=Dense(3)) # Ponemos 3 porque hemos querido poner 3 neuronas (se pueden\n",
    "# modificar y poner los que se quiera)\n",
    "model.add(layer=Dense(1, )) # Este ser√≠a la variable objetiva (el total)\n",
    "# Ni el input ni el dense final se pueden modificar, si no fallar√≠a el modelo\n",
    "\n",
    "# 3) Definimos las variables explicativas y objetiva\n",
    "X = df.drop(columns='total')\n",
    "y = df.total\n",
    "\n",
    "# 4) Ajustamos el modelo y lo lanzamos\n",
    "model.compile(loss='mse', metrics=['mse'])\n",
    "model.fit(X, y, epochs=500, verbose=2) # el verbose permite ver el loss y el mse que\n",
    "# definimos justo arriba de esta funci√≥n y que se ve en el lanzamiento\n",
    "\n",
    "# 5) Observamos los n√∫meros para los weights\n",
    "model.get_weights()\n",
    "\n",
    "# 6) Comparamos con los datos del dataset\n",
    "y_pred = model.predict(X)\n",
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>22.246986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>18.858179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.835787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>18.440342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>16.650234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1\n",
       "abbrev                    \n",
       "AL       18.8    22.246986\n",
       "AK       18.1    18.858179\n",
       "AZ       18.6    16.835787\n",
       "AR       22.4    18.440342\n",
       "CA       12.0    16.650234"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL = X[:1]\n",
    "\n",
    "model.predict(x=AL)\n",
    "model.get_weights()\n",
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_init_1'] = model.predict(X)\n",
    "dfsel.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to `glorot_uniform` (default)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use `sigmoid` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid')) # aqu√≠ se realiza el cambio\n",
    "\n",
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 2/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 3/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 4/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 5/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 6/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 7/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 8/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 9/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 10/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 11/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:07:22.384292: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 13/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 14/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 15/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 16/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 17/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 18/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 19/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 20/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 21/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 22/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 23/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 24/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 25/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 26/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 27/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 28/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 29/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 30/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 31/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 32/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 33/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 34/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 35/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 36/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 37/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 38/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 39/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 40/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 41/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 42/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 43/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 44/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 45/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 46/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 47/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 48/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 49/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 50/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 51/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 52/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 53/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 54/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 55/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 56/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 57/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 58/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 59/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 60/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 61/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 62/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 63/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 64/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 65/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 66/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 67/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 68/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 69/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 70/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 71/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 72/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 73/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 74/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 75/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 76/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 77/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 78/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 79/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 80/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 81/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 82/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 83/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 84/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 85/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 86/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 87/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 88/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 89/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 90/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 91/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 92/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 93/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 94/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 95/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 96/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 97/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 98/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 99/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 100/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 101/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 102/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 103/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 104/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 105/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 106/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 107/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 108/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 109/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 110/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 111/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 112/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 113/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 114/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 115/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 116/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 117/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 118/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 119/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 120/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 121/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 122/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 123/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 124/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 125/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 126/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 127/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 128/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 129/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 130/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 131/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 132/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 133/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 134/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 135/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 136/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 137/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 138/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 139/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 140/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 141/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 142/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 143/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 144/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 145/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 146/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 147/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 148/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 149/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 150/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 151/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 152/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 153/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 154/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 155/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 156/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 157/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 158/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 159/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 160/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 161/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 162/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 163/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 164/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 165/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 166/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 167/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 168/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 169/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 170/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 171/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 172/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 173/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 174/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 175/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 176/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 177/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 178/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 179/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 180/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 181/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 182/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 183/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 184/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 185/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 186/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 187/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 188/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 189/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 190/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 191/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 192/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 193/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 194/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 195/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 196/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 197/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 198/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 199/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 200/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 201/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 202/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 203/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 204/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 205/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 206/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 207/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 208/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 209/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 210/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 211/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 212/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 213/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 214/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 215/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 216/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 217/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 218/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 219/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 220/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 221/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 222/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 223/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 224/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 225/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 226/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 227/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 228/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 229/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 230/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 231/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 232/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 233/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 234/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 235/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 236/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 237/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 238/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 239/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 240/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 241/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 242/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 243/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 244/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 245/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 246/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 247/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 248/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 249/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 250/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 251/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 252/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 253/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 254/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 255/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 256/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 257/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 258/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 259/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 260/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 261/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 262/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 263/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 264/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 265/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 266/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 267/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 268/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 269/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 270/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 271/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 272/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 273/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 274/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 275/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 276/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 277/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 278/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 279/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 280/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 281/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 282/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 283/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 284/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 285/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 286/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 287/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 288/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 289/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 290/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 291/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 292/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 293/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 294/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 295/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 296/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 297/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 298/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 299/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 300/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 301/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 302/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 303/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 304/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 305/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 306/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 307/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 308/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 309/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 310/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 311/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 312/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 313/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 314/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 315/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 316/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 317/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 318/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 319/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 320/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 321/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 322/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 323/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 324/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 325/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 326/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 327/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 328/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 329/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 330/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 331/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 332/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 333/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 334/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 335/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 336/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 337/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 338/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 339/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 340/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 341/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 342/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 343/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 344/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 345/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 346/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 347/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 348/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 349/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 350/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 351/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 352/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 353/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 354/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 355/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 356/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 357/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 358/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 359/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 360/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 361/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 362/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 363/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 364/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 365/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 366/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 367/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 368/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 369/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 370/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 371/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 372/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 373/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 374/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 375/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 376/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 377/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 378/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 379/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 380/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 381/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 382/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 383/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 384/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 385/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 386/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 387/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 388/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 389/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 390/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 391/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 392/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 393/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 394/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 395/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 396/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 397/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 398/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 399/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 400/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 401/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 402/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 403/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 404/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 405/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 406/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 407/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 408/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 409/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 410/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 411/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 412/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 413/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 414/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 415/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 416/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 417/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 418/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 419/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 420/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 421/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 422/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 423/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 424/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 425/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 426/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 427/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 428/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 429/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 430/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 431/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 432/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 433/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 434/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 435/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 436/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 437/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 438/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 439/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 440/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 441/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 442/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 443/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 444/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 445/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 446/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 447/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 448/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 449/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 450/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 451/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 452/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 453/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 454/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 455/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 456/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 457/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 458/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 459/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 460/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 461/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 462/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 463/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 464/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 465/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 466/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 467/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 468/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 469/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 470/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 471/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 472/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 473/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 474/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 475/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 476/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 477/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 478/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 479/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 480/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 481/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 482/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 483/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4076\n",
      "Epoch 484/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 485/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 486/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 487/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 488/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 489/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 490/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 491/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 492/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 493/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 494/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 495/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 496/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 497/500\n",
      "2/2 - 0s - loss: 235.4077 - mse: 235.4077\n",
      "Epoch 498/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 499/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n",
      "Epoch 500/500\n",
      "2/2 - 0s - loss: 235.4076 - mse: 235.4076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1774b70d0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x16d66ce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:13:18.079634: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>22.246986</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>18.858179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.835787</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>18.440342</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>16.650234</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid\n",
       "abbrev                                  \n",
       "AL       18.8    22.246986           1.0\n",
       "AK       18.1    18.858179           1.0\n",
       "AZ       18.6    16.835787           1.0\n",
       "AR       22.4    18.440342           1.0\n",
       "CA       12.0    16.650234           1.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_sigmoid'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.68471324, -0.8127401 , -0.25871682],\n",
       "        [-0.48054293,  0.16895252,  0.11927074],\n",
       "        [ 0.28029478,  0.71608007, -0.0089851 ],\n",
       "        [ 0.0437851 ,  0.04770046,  0.12211698],\n",
       "        [ 0.10078061,  0.71092236,  0.5532018 ],\n",
       "        [-0.7621052 ,  0.47481048,  0.71016645]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.5362694],\n",
       "        [-0.570507 ],\n",
       "        [ 0.856423 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `linear` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 - 0s - loss: 236174.5156 - mse: 236174.5156\n",
      "Epoch 2/500\n",
      "2/2 - 0s - loss: 223631.2188 - mse: 223631.2188\n",
      "Epoch 3/500\n",
      "2/2 - 0s - loss: 215081.0469 - mse: 215081.0469\n",
      "Epoch 4/500\n",
      "2/2 - 0s - loss: 208049.1406 - mse: 208049.1406\n",
      "Epoch 5/500\n",
      "2/2 - 0s - loss: 201765.6406 - mse: 201765.6094\n",
      "Epoch 6/500\n",
      "2/2 - 0s - loss: 196025.3594 - mse: 196025.3594\n",
      "Epoch 7/500\n",
      "2/2 - 0s - loss: 190695.9844 - mse: 190695.9844\n",
      "Epoch 8/500\n",
      "2/2 - 0s - loss: 185707.5781 - mse: 185707.5781\n",
      "Epoch 9/500\n",
      "2/2 - 0s - loss: 180938.4375 - mse: 180938.4375\n",
      "Epoch 10/500\n",
      "2/2 - 0s - loss: 176303.9844 - mse: 176303.9844\n",
      "Epoch 11/500\n",
      "2/2 - 0s - loss: 171939.2656 - mse: 171939.2656\n",
      "Epoch 12/500\n",
      "2/2 - 0s - loss: 167674.7344 - mse: 167674.7344\n",
      "Epoch 13/500\n",
      "2/2 - 0s - loss: 163529.5938 - mse: 163529.5938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:13:39.331777: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "2/2 - 0s - loss: 159411.1562 - mse: 159411.1562\n",
      "Epoch 15/500\n",
      "2/2 - 0s - loss: 155367.4531 - mse: 155367.4531\n",
      "Epoch 16/500\n",
      "2/2 - 0s - loss: 151435.8281 - mse: 151435.8281\n",
      "Epoch 17/500\n",
      "2/2 - 0s - loss: 147594.3438 - mse: 147594.3438\n",
      "Epoch 18/500\n",
      "2/2 - 0s - loss: 143841.2500 - mse: 143841.2500\n",
      "Epoch 19/500\n",
      "2/2 - 0s - loss: 140160.4219 - mse: 140160.4219\n",
      "Epoch 20/500\n",
      "2/2 - 0s - loss: 136471.7656 - mse: 136471.7656\n",
      "Epoch 21/500\n",
      "2/2 - 0s - loss: 132881.3594 - mse: 132881.3594\n",
      "Epoch 22/500\n",
      "2/2 - 0s - loss: 129360.3281 - mse: 129360.3281\n",
      "Epoch 23/500\n",
      "2/2 - 0s - loss: 125905.8203 - mse: 125905.8203\n",
      "Epoch 24/500\n",
      "2/2 - 0s - loss: 122502.9297 - mse: 122502.9297\n",
      "Epoch 25/500\n",
      "2/2 - 0s - loss: 119156.7188 - mse: 119156.7188\n",
      "Epoch 26/500\n",
      "2/2 - 0s - loss: 115849.2422 - mse: 115849.2422\n",
      "Epoch 27/500\n",
      "2/2 - 0s - loss: 112629.3750 - mse: 112629.3750\n",
      "Epoch 28/500\n",
      "2/2 - 0s - loss: 109485.8281 - mse: 109485.8281\n",
      "Epoch 29/500\n",
      "2/2 - 0s - loss: 106394.5234 - mse: 106394.5234\n",
      "Epoch 30/500\n",
      "2/2 - 0s - loss: 103326.8047 - mse: 103326.8047\n",
      "Epoch 31/500\n",
      "2/2 - 0s - loss: 100294.9531 - mse: 100294.9531\n",
      "Epoch 32/500\n",
      "2/2 - 0s - loss: 97323.6484 - mse: 97323.6484\n",
      "Epoch 33/500\n",
      "2/2 - 0s - loss: 94400.3828 - mse: 94400.3828\n",
      "Epoch 34/500\n",
      "2/2 - 0s - loss: 91540.0625 - mse: 91540.0625\n",
      "Epoch 35/500\n",
      "2/2 - 0s - loss: 88701.8438 - mse: 88701.8438\n",
      "Epoch 36/500\n",
      "2/2 - 0s - loss: 85891.3047 - mse: 85891.3047\n",
      "Epoch 37/500\n",
      "2/2 - 0s - loss: 83189.9062 - mse: 83189.9062\n",
      "Epoch 38/500\n",
      "2/2 - 0s - loss: 80562.3047 - mse: 80562.3047\n",
      "Epoch 39/500\n",
      "2/2 - 0s - loss: 77975.4375 - mse: 77975.4375\n",
      "Epoch 40/500\n",
      "2/2 - 0s - loss: 75450.1641 - mse: 75450.1641\n",
      "Epoch 41/500\n",
      "2/2 - 0s - loss: 72942.4609 - mse: 72942.4609\n",
      "Epoch 42/500\n",
      "2/2 - 0s - loss: 70465.4766 - mse: 70465.4766\n",
      "Epoch 43/500\n",
      "2/2 - 0s - loss: 68008.6406 - mse: 68008.6406\n",
      "Epoch 44/500\n",
      "2/2 - 0s - loss: 65618.2344 - mse: 65618.2344\n",
      "Epoch 45/500\n",
      "2/2 - 0s - loss: 63300.6484 - mse: 63300.6484\n",
      "Epoch 46/500\n",
      "2/2 - 0s - loss: 61052.9297 - mse: 61052.9297\n",
      "Epoch 47/500\n",
      "2/2 - 0s - loss: 58852.6211 - mse: 58852.6211\n",
      "Epoch 48/500\n",
      "2/2 - 0s - loss: 56769.4766 - mse: 56769.4766\n",
      "Epoch 49/500\n",
      "2/2 - 0s - loss: 54702.2617 - mse: 54702.2617\n",
      "Epoch 50/500\n",
      "2/2 - 0s - loss: 52627.3555 - mse: 52627.3555\n",
      "Epoch 51/500\n",
      "2/2 - 0s - loss: 50578.4766 - mse: 50578.4766\n",
      "Epoch 52/500\n",
      "2/2 - 0s - loss: 48572.1484 - mse: 48572.1484\n",
      "Epoch 53/500\n",
      "2/2 - 0s - loss: 46631.7461 - mse: 46631.7461\n",
      "Epoch 54/500\n",
      "2/2 - 0s - loss: 44774.4336 - mse: 44774.4336\n",
      "Epoch 55/500\n",
      "2/2 - 0s - loss: 42956.9141 - mse: 42956.9141\n",
      "Epoch 56/500\n",
      "2/2 - 0s - loss: 41115.5312 - mse: 41115.5312\n",
      "Epoch 57/500\n",
      "2/2 - 0s - loss: 39329.2578 - mse: 39329.2578\n",
      "Epoch 58/500\n",
      "2/2 - 0s - loss: 37628.7812 - mse: 37628.7812\n",
      "Epoch 59/500\n",
      "2/2 - 0s - loss: 35947.6992 - mse: 35947.6992\n",
      "Epoch 60/500\n",
      "2/2 - 0s - loss: 34288.7031 - mse: 34288.7031\n",
      "Epoch 61/500\n",
      "2/2 - 0s - loss: 32692.9863 - mse: 32692.9844\n",
      "Epoch 62/500\n",
      "2/2 - 0s - loss: 31161.1992 - mse: 31161.1992\n",
      "Epoch 63/500\n",
      "2/2 - 0s - loss: 29662.5020 - mse: 29662.4980\n",
      "Epoch 64/500\n",
      "2/2 - 0s - loss: 28169.2109 - mse: 28169.2109\n",
      "Epoch 65/500\n",
      "2/2 - 0s - loss: 26779.0430 - mse: 26779.0430\n",
      "Epoch 66/500\n",
      "2/2 - 0s - loss: 25444.0703 - mse: 25444.0703\n",
      "Epoch 67/500\n",
      "2/2 - 0s - loss: 24123.7949 - mse: 24123.7949\n",
      "Epoch 68/500\n",
      "2/2 - 0s - loss: 22821.4316 - mse: 22821.4316\n",
      "Epoch 69/500\n",
      "2/2 - 0s - loss: 21568.0059 - mse: 21568.0059\n",
      "Epoch 70/500\n",
      "2/2 - 0s - loss: 20338.7168 - mse: 20338.7168\n",
      "Epoch 71/500\n",
      "2/2 - 0s - loss: 19190.4785 - mse: 19190.4785\n",
      "Epoch 72/500\n",
      "2/2 - 0s - loss: 18092.4707 - mse: 18092.4707\n",
      "Epoch 73/500\n",
      "2/2 - 0s - loss: 17019.7520 - mse: 17019.7520\n",
      "Epoch 74/500\n",
      "2/2 - 0s - loss: 15982.0225 - mse: 15982.0225\n",
      "Epoch 75/500\n",
      "2/2 - 0s - loss: 14987.6025 - mse: 14987.6025\n",
      "Epoch 76/500\n",
      "2/2 - 0s - loss: 14022.7451 - mse: 14022.7451\n",
      "Epoch 77/500\n",
      "2/2 - 0s - loss: 13087.8730 - mse: 13087.8730\n",
      "Epoch 78/500\n",
      "2/2 - 0s - loss: 12195.1279 - mse: 12195.1279\n",
      "Epoch 79/500\n",
      "2/2 - 0s - loss: 11345.8740 - mse: 11345.8740\n",
      "Epoch 80/500\n",
      "2/2 - 0s - loss: 10525.2773 - mse: 10525.2773\n",
      "Epoch 81/500\n",
      "2/2 - 0s - loss: 9742.7451 - mse: 9742.7451\n",
      "Epoch 82/500\n",
      "2/2 - 0s - loss: 9008.8857 - mse: 9008.8857\n",
      "Epoch 83/500\n",
      "2/2 - 0s - loss: 8297.5752 - mse: 8297.5752\n",
      "Epoch 84/500\n",
      "2/2 - 0s - loss: 7620.2808 - mse: 7620.2808\n",
      "Epoch 85/500\n",
      "2/2 - 0s - loss: 6975.4673 - mse: 6975.4673\n",
      "Epoch 86/500\n",
      "2/2 - 0s - loss: 6360.1597 - mse: 6360.1597\n",
      "Epoch 87/500\n",
      "2/2 - 0s - loss: 5769.2134 - mse: 5769.2134\n",
      "Epoch 88/500\n",
      "2/2 - 0s - loss: 5227.1670 - mse: 5227.1670\n",
      "Epoch 89/500\n",
      "2/2 - 0s - loss: 4726.4712 - mse: 4726.4712\n",
      "Epoch 90/500\n",
      "2/2 - 0s - loss: 4249.7939 - mse: 4249.7939\n",
      "Epoch 91/500\n",
      "2/2 - 0s - loss: 3798.4551 - mse: 3798.4551\n",
      "Epoch 92/500\n",
      "2/2 - 0s - loss: 3385.6067 - mse: 3385.6067\n",
      "Epoch 93/500\n",
      "2/2 - 0s - loss: 3002.2783 - mse: 3002.2783\n",
      "Epoch 94/500\n",
      "2/2 - 0s - loss: 2653.3655 - mse: 2653.3655\n",
      "Epoch 95/500\n",
      "2/2 - 0s - loss: 2322.8445 - mse: 2322.8445\n",
      "Epoch 96/500\n",
      "2/2 - 0s - loss: 2019.8625 - mse: 2019.8625\n",
      "Epoch 97/500\n",
      "2/2 - 0s - loss: 1755.2250 - mse: 1755.2250\n",
      "Epoch 98/500\n",
      "2/2 - 0s - loss: 1512.4128 - mse: 1512.4128\n",
      "Epoch 99/500\n",
      "2/2 - 0s - loss: 1296.3286 - mse: 1296.3286\n",
      "Epoch 100/500\n",
      "2/2 - 0s - loss: 1103.5399 - mse: 1103.5399\n",
      "Epoch 101/500\n",
      "2/2 - 0s - loss: 929.2483 - mse: 929.2483\n",
      "Epoch 102/500\n",
      "2/2 - 0s - loss: 776.9663 - mse: 776.9663\n",
      "Epoch 103/500\n",
      "2/2 - 0s - loss: 646.8909 - mse: 646.8909\n",
      "Epoch 104/500\n",
      "2/2 - 0s - loss: 532.1398 - mse: 532.1398\n",
      "Epoch 105/500\n",
      "2/2 - 0s - loss: 435.4065 - mse: 435.4064\n",
      "Epoch 106/500\n",
      "2/2 - 0s - loss: 353.9453 - mse: 353.9453\n",
      "Epoch 107/500\n",
      "2/2 - 0s - loss: 292.4769 - mse: 292.4769\n",
      "Epoch 108/500\n",
      "2/2 - 0s - loss: 242.8091 - mse: 242.8091\n",
      "Epoch 109/500\n",
      "2/2 - 0s - loss: 203.7980 - mse: 203.7980\n",
      "Epoch 110/500\n",
      "2/2 - 0s - loss: 175.3403 - mse: 175.3403\n",
      "Epoch 111/500\n",
      "2/2 - 0s - loss: 154.1314 - mse: 154.1314\n",
      "Epoch 112/500\n",
      "2/2 - 0s - loss: 138.8790 - mse: 138.8790\n",
      "Epoch 113/500\n",
      "2/2 - 0s - loss: 128.0481 - mse: 128.0481\n",
      "Epoch 114/500\n",
      "2/2 - 0s - loss: 121.7238 - mse: 121.7238\n",
      "Epoch 115/500\n",
      "2/2 - 0s - loss: 117.9364 - mse: 117.9364\n",
      "Epoch 116/500\n",
      "2/2 - 0s - loss: 114.8464 - mse: 114.8464\n",
      "Epoch 117/500\n",
      "2/2 - 0s - loss: 113.7263 - mse: 113.7263\n",
      "Epoch 118/500\n",
      "2/2 - 0s - loss: 112.7070 - mse: 112.7070\n",
      "Epoch 119/500\n",
      "2/2 - 0s - loss: 112.8387 - mse: 112.8387\n",
      "Epoch 120/500\n",
      "2/2 - 0s - loss: 111.9974 - mse: 111.9974\n",
      "Epoch 121/500\n",
      "2/2 - 0s - loss: 111.5703 - mse: 111.5703\n",
      "Epoch 122/500\n",
      "2/2 - 0s - loss: 111.2336 - mse: 111.2336\n",
      "Epoch 123/500\n",
      "2/2 - 0s - loss: 110.9145 - mse: 110.9145\n",
      "Epoch 124/500\n",
      "2/2 - 0s - loss: 110.4502 - mse: 110.4502\n",
      "Epoch 125/500\n",
      "2/2 - 0s - loss: 113.2528 - mse: 113.2528\n",
      "Epoch 126/500\n",
      "2/2 - 0s - loss: 111.0868 - mse: 111.0868\n",
      "Epoch 127/500\n",
      "2/2 - 0s - loss: 109.1406 - mse: 109.1406\n",
      "Epoch 128/500\n",
      "2/2 - 0s - loss: 109.6770 - mse: 109.6770\n",
      "Epoch 129/500\n",
      "2/2 - 0s - loss: 109.4316 - mse: 109.4316\n",
      "Epoch 130/500\n",
      "2/2 - 0s - loss: 108.4432 - mse: 108.4432\n",
      "Epoch 131/500\n",
      "2/2 - 0s - loss: 107.6283 - mse: 107.6283\n",
      "Epoch 132/500\n",
      "2/2 - 0s - loss: 106.3523 - mse: 106.3523\n",
      "Epoch 133/500\n",
      "2/2 - 0s - loss: 107.1049 - mse: 107.1049\n",
      "Epoch 134/500\n",
      "2/2 - 0s - loss: 105.8076 - mse: 105.8076\n",
      "Epoch 135/500\n",
      "2/2 - 0s - loss: 108.9538 - mse: 108.9538\n",
      "Epoch 136/500\n",
      "2/2 - 0s - loss: 110.5093 - mse: 110.5093\n",
      "Epoch 137/500\n",
      "2/2 - 0s - loss: 106.3180 - mse: 106.3180\n",
      "Epoch 138/500\n",
      "2/2 - 0s - loss: 101.8785 - mse: 101.8785\n",
      "Epoch 139/500\n",
      "2/2 - 0s - loss: 101.1746 - mse: 101.1746\n",
      "Epoch 140/500\n",
      "2/2 - 0s - loss: 100.7633 - mse: 100.7633\n",
      "Epoch 141/500\n",
      "2/2 - 0s - loss: 102.0089 - mse: 102.0089\n",
      "Epoch 142/500\n",
      "2/2 - 0s - loss: 106.4188 - mse: 106.4188\n",
      "Epoch 143/500\n",
      "2/2 - 0s - loss: 110.6041 - mse: 110.6041\n",
      "Epoch 144/500\n",
      "2/2 - 0s - loss: 99.6814 - mse: 99.6814\n",
      "Epoch 145/500\n",
      "2/2 - 0s - loss: 103.4579 - mse: 103.4579\n",
      "Epoch 146/500\n",
      "2/2 - 0s - loss: 100.7213 - mse: 100.7213\n",
      "Epoch 147/500\n",
      "2/2 - 0s - loss: 100.7297 - mse: 100.7297\n",
      "Epoch 148/500\n",
      "2/2 - 0s - loss: 100.0028 - mse: 100.0028\n",
      "Epoch 149/500\n",
      "2/2 - 0s - loss: 106.2358 - mse: 106.2358\n",
      "Epoch 150/500\n",
      "2/2 - 0s - loss: 97.2729 - mse: 97.2729\n",
      "Epoch 151/500\n",
      "2/2 - 0s - loss: 93.6031 - mse: 93.6031\n",
      "Epoch 152/500\n",
      "2/2 - 0s - loss: 100.7537 - mse: 100.7537\n",
      "Epoch 153/500\n",
      "2/2 - 0s - loss: 97.1262 - mse: 97.1262\n",
      "Epoch 154/500\n",
      "2/2 - 0s - loss: 92.9538 - mse: 92.9538\n",
      "Epoch 155/500\n",
      "2/2 - 0s - loss: 94.3311 - mse: 94.3311\n",
      "Epoch 156/500\n",
      "2/2 - 0s - loss: 92.1105 - mse: 92.1105\n",
      "Epoch 157/500\n",
      "2/2 - 0s - loss: 92.7138 - mse: 92.7138\n",
      "Epoch 158/500\n",
      "2/2 - 0s - loss: 95.4682 - mse: 95.4682\n",
      "Epoch 159/500\n",
      "2/2 - 0s - loss: 90.5814 - mse: 90.5814\n",
      "Epoch 160/500\n",
      "2/2 - 0s - loss: 89.1048 - mse: 89.1048\n",
      "Epoch 161/500\n",
      "2/2 - 0s - loss: 88.8177 - mse: 88.8177\n",
      "Epoch 162/500\n",
      "2/2 - 0s - loss: 89.4145 - mse: 89.4145\n",
      "Epoch 163/500\n",
      "2/2 - 0s - loss: 86.7345 - mse: 86.7345\n",
      "Epoch 164/500\n",
      "2/2 - 0s - loss: 88.2248 - mse: 88.2248\n",
      "Epoch 165/500\n",
      "2/2 - 0s - loss: 87.7113 - mse: 87.7113\n",
      "Epoch 166/500\n",
      "2/2 - 0s - loss: 86.6158 - mse: 86.6158\n",
      "Epoch 167/500\n",
      "2/2 - 0s - loss: 83.9169 - mse: 83.9169\n",
      "Epoch 168/500\n",
      "2/2 - 0s - loss: 82.8319 - mse: 82.8319\n",
      "Epoch 169/500\n",
      "2/2 - 0s - loss: 82.0804 - mse: 82.0804\n",
      "Epoch 170/500\n",
      "2/2 - 0s - loss: 84.3700 - mse: 84.3700\n",
      "Epoch 171/500\n",
      "2/2 - 0s - loss: 85.1726 - mse: 85.1726\n",
      "Epoch 172/500\n",
      "2/2 - 0s - loss: 80.1829 - mse: 80.1829\n",
      "Epoch 173/500\n",
      "2/2 - 0s - loss: 81.0484 - mse: 81.0484\n",
      "Epoch 174/500\n",
      "2/2 - 0s - loss: 96.0412 - mse: 96.0412\n",
      "Epoch 175/500\n",
      "2/2 - 0s - loss: 82.1336 - mse: 82.1336\n",
      "Epoch 176/500\n",
      "2/2 - 0s - loss: 78.4560 - mse: 78.4560\n",
      "Epoch 177/500\n",
      "2/2 - 0s - loss: 77.1784 - mse: 77.1784\n",
      "Epoch 178/500\n",
      "2/2 - 0s - loss: 76.5730 - mse: 76.5730\n",
      "Epoch 179/500\n",
      "2/2 - 0s - loss: 82.3860 - mse: 82.3860\n",
      "Epoch 180/500\n",
      "2/2 - 0s - loss: 78.2081 - mse: 78.2081\n",
      "Epoch 181/500\n",
      "2/2 - 0s - loss: 77.7641 - mse: 77.7641\n",
      "Epoch 182/500\n",
      "2/2 - 0s - loss: 80.0777 - mse: 80.0777\n",
      "Epoch 183/500\n",
      "2/2 - 0s - loss: 76.7161 - mse: 76.7161\n",
      "Epoch 184/500\n",
      "2/2 - 0s - loss: 76.6449 - mse: 76.6449\n",
      "Epoch 185/500\n",
      "2/2 - 0s - loss: 73.4630 - mse: 73.4630\n",
      "Epoch 186/500\n",
      "2/2 - 0s - loss: 72.7846 - mse: 72.7846\n",
      "Epoch 187/500\n",
      "2/2 - 0s - loss: 73.9208 - mse: 73.9208\n",
      "Epoch 188/500\n",
      "2/2 - 0s - loss: 72.4663 - mse: 72.4663\n",
      "Epoch 189/500\n",
      "2/2 - 0s - loss: 70.6052 - mse: 70.6052\n",
      "Epoch 190/500\n",
      "2/2 - 0s - loss: 69.7439 - mse: 69.7439\n",
      "Epoch 191/500\n",
      "2/2 - 0s - loss: 71.8183 - mse: 71.8183\n",
      "Epoch 192/500\n",
      "2/2 - 0s - loss: 69.3209 - mse: 69.3209\n",
      "Epoch 193/500\n",
      "2/2 - 0s - loss: 68.8751 - mse: 68.8751\n",
      "Epoch 194/500\n",
      "2/2 - 0s - loss: 68.6151 - mse: 68.6151\n",
      "Epoch 195/500\n",
      "2/2 - 0s - loss: 67.3834 - mse: 67.3834\n",
      "Epoch 196/500\n",
      "2/2 - 0s - loss: 67.5748 - mse: 67.5748\n",
      "Epoch 197/500\n",
      "2/2 - 0s - loss: 67.6127 - mse: 67.6127\n",
      "Epoch 198/500\n",
      "2/2 - 0s - loss: 67.7571 - mse: 67.7571\n",
      "Epoch 199/500\n",
      "2/2 - 0s - loss: 70.1841 - mse: 70.1841\n",
      "Epoch 200/500\n",
      "2/2 - 0s - loss: 66.4350 - mse: 66.4350\n",
      "Epoch 201/500\n",
      "2/2 - 0s - loss: 63.4192 - mse: 63.4192\n",
      "Epoch 202/500\n",
      "2/2 - 0s - loss: 65.9109 - mse: 65.9109\n",
      "Epoch 203/500\n",
      "2/2 - 0s - loss: 72.5992 - mse: 72.5992\n",
      "Epoch 204/500\n",
      "2/2 - 0s - loss: 63.7118 - mse: 63.7118\n",
      "Epoch 205/500\n",
      "2/2 - 0s - loss: 64.8461 - mse: 64.8461\n",
      "Epoch 206/500\n",
      "2/2 - 0s - loss: 64.0661 - mse: 64.0661\n",
      "Epoch 207/500\n",
      "2/2 - 0s - loss: 67.1794 - mse: 67.1794\n",
      "Epoch 208/500\n",
      "2/2 - 0s - loss: 62.2051 - mse: 62.2051\n",
      "Epoch 209/500\n",
      "2/2 - 0s - loss: 61.4046 - mse: 61.4046\n",
      "Epoch 210/500\n",
      "2/2 - 0s - loss: 59.2042 - mse: 59.2042\n",
      "Epoch 211/500\n",
      "2/2 - 0s - loss: 59.2957 - mse: 59.2957\n",
      "Epoch 212/500\n",
      "2/2 - 0s - loss: 62.5109 - mse: 62.5109\n",
      "Epoch 213/500\n",
      "2/2 - 0s - loss: 68.5339 - mse: 68.5339\n",
      "Epoch 214/500\n",
      "2/2 - 0s - loss: 59.6424 - mse: 59.6424\n",
      "Epoch 215/500\n",
      "2/2 - 0s - loss: 57.6440 - mse: 57.6440\n",
      "Epoch 216/500\n",
      "2/2 - 0s - loss: 58.1186 - mse: 58.1186\n",
      "Epoch 217/500\n",
      "2/2 - 0s - loss: 57.4727 - mse: 57.4727\n",
      "Epoch 218/500\n",
      "2/2 - 0s - loss: 56.5705 - mse: 56.5705\n",
      "Epoch 219/500\n",
      "2/2 - 0s - loss: 55.8050 - mse: 55.8050\n",
      "Epoch 220/500\n",
      "2/2 - 0s - loss: 60.1206 - mse: 60.1206\n",
      "Epoch 221/500\n",
      "2/2 - 0s - loss: 57.4156 - mse: 57.4156\n",
      "Epoch 222/500\n",
      "2/2 - 0s - loss: 57.2385 - mse: 57.2385\n",
      "Epoch 223/500\n",
      "2/2 - 0s - loss: 53.9749 - mse: 53.9749\n",
      "Epoch 224/500\n",
      "2/2 - 0s - loss: 53.7990 - mse: 53.7990\n",
      "Epoch 225/500\n",
      "2/2 - 0s - loss: 52.9999 - mse: 52.9999\n",
      "Epoch 226/500\n",
      "2/2 - 0s - loss: 56.0927 - mse: 56.0927\n",
      "Epoch 227/500\n",
      "2/2 - 0s - loss: 55.3156 - mse: 55.3156\n",
      "Epoch 228/500\n",
      "2/2 - 0s - loss: 53.2090 - mse: 53.2090\n",
      "Epoch 229/500\n",
      "2/2 - 0s - loss: 51.7627 - mse: 51.7627\n",
      "Epoch 230/500\n",
      "2/2 - 0s - loss: 51.6154 - mse: 51.6154\n",
      "Epoch 231/500\n",
      "2/2 - 0s - loss: 53.0116 - mse: 53.0116\n",
      "Epoch 232/500\n",
      "2/2 - 0s - loss: 50.0446 - mse: 50.0446\n",
      "Epoch 233/500\n",
      "2/2 - 0s - loss: 55.7984 - mse: 55.7984\n",
      "Epoch 234/500\n",
      "2/2 - 0s - loss: 52.1030 - mse: 52.1030\n",
      "Epoch 235/500\n",
      "2/2 - 0s - loss: 50.7223 - mse: 50.7223\n",
      "Epoch 236/500\n",
      "2/2 - 0s - loss: 49.0596 - mse: 49.0596\n",
      "Epoch 237/500\n",
      "2/2 - 0s - loss: 49.2234 - mse: 49.2234\n",
      "Epoch 238/500\n",
      "2/2 - 0s - loss: 50.1472 - mse: 50.1472\n",
      "Epoch 239/500\n",
      "2/2 - 0s - loss: 47.1705 - mse: 47.1705\n",
      "Epoch 240/500\n",
      "2/2 - 0s - loss: 46.7987 - mse: 46.7987\n",
      "Epoch 241/500\n",
      "2/2 - 0s - loss: 46.8171 - mse: 46.8171\n",
      "Epoch 242/500\n",
      "2/2 - 0s - loss: 49.6855 - mse: 49.6855\n",
      "Epoch 243/500\n",
      "2/2 - 0s - loss: 50.6036 - mse: 50.6036\n",
      "Epoch 244/500\n",
      "2/2 - 0s - loss: 47.2064 - mse: 47.2064\n",
      "Epoch 245/500\n",
      "2/2 - 0s - loss: 45.7782 - mse: 45.7782\n",
      "Epoch 246/500\n",
      "2/2 - 0s - loss: 50.2907 - mse: 50.2907\n",
      "Epoch 247/500\n",
      "2/2 - 0s - loss: 45.0513 - mse: 45.0513\n",
      "Epoch 248/500\n",
      "2/2 - 0s - loss: 43.6283 - mse: 43.6283\n",
      "Epoch 249/500\n",
      "2/2 - 0s - loss: 43.7349 - mse: 43.7349\n",
      "Epoch 250/500\n",
      "2/2 - 0s - loss: 43.1423 - mse: 43.1423\n",
      "Epoch 251/500\n",
      "2/2 - 0s - loss: 50.9233 - mse: 50.9233\n",
      "Epoch 252/500\n",
      "2/2 - 0s - loss: 53.0979 - mse: 53.0979\n",
      "Epoch 253/500\n",
      "2/2 - 0s - loss: 43.7783 - mse: 43.7783\n",
      "Epoch 254/500\n",
      "2/2 - 0s - loss: 42.3372 - mse: 42.3372\n",
      "Epoch 255/500\n",
      "2/2 - 0s - loss: 44.0575 - mse: 44.0575\n",
      "Epoch 256/500\n",
      "2/2 - 0s - loss: 45.6824 - mse: 45.6824\n",
      "Epoch 257/500\n",
      "2/2 - 0s - loss: 41.6358 - mse: 41.6358\n",
      "Epoch 258/500\n",
      "2/2 - 0s - loss: 41.6375 - mse: 41.6375\n",
      "Epoch 259/500\n",
      "2/2 - 0s - loss: 40.2698 - mse: 40.2698\n",
      "Epoch 260/500\n",
      "2/2 - 0s - loss: 40.0328 - mse: 40.0328\n",
      "Epoch 261/500\n",
      "2/2 - 0s - loss: 39.9305 - mse: 39.9305\n",
      "Epoch 262/500\n",
      "2/2 - 0s - loss: 40.4195 - mse: 40.4195\n",
      "Epoch 263/500\n",
      "2/2 - 0s - loss: 44.3210 - mse: 44.3210\n",
      "Epoch 264/500\n",
      "2/2 - 0s - loss: 39.2513 - mse: 39.2513\n",
      "Epoch 265/500\n",
      "2/2 - 0s - loss: 41.3889 - mse: 41.3889\n",
      "Epoch 266/500\n",
      "2/2 - 0s - loss: 39.3150 - mse: 39.3150\n",
      "Epoch 267/500\n",
      "2/2 - 0s - loss: 37.7609 - mse: 37.7609\n",
      "Epoch 268/500\n",
      "2/2 - 0s - loss: 39.9651 - mse: 39.9651\n",
      "Epoch 269/500\n",
      "2/2 - 0s - loss: 47.3082 - mse: 47.3082\n",
      "Epoch 270/500\n",
      "2/2 - 0s - loss: 40.0899 - mse: 40.0899\n",
      "Epoch 271/500\n",
      "2/2 - 0s - loss: 36.7062 - mse: 36.7062\n",
      "Epoch 272/500\n",
      "2/2 - 0s - loss: 36.8350 - mse: 36.8350\n",
      "Epoch 273/500\n",
      "2/2 - 0s - loss: 36.5005 - mse: 36.5005\n",
      "Epoch 274/500\n",
      "2/2 - 0s - loss: 36.8050 - mse: 36.8050\n",
      "Epoch 275/500\n",
      "2/2 - 0s - loss: 35.9330 - mse: 35.9330\n",
      "Epoch 276/500\n",
      "2/2 - 0s - loss: 35.7260 - mse: 35.7260\n",
      "Epoch 277/500\n",
      "2/2 - 0s - loss: 34.9756 - mse: 34.9756\n",
      "Epoch 278/500\n",
      "2/2 - 0s - loss: 36.1862 - mse: 36.1862\n",
      "Epoch 279/500\n",
      "2/2 - 0s - loss: 34.6324 - mse: 34.6324\n",
      "Epoch 280/500\n",
      "2/2 - 0s - loss: 35.0971 - mse: 35.0971\n",
      "Epoch 281/500\n",
      "2/2 - 0s - loss: 40.5506 - mse: 40.5506\n",
      "Epoch 282/500\n",
      "2/2 - 0s - loss: 38.4983 - mse: 38.4983\n",
      "Epoch 283/500\n",
      "2/2 - 0s - loss: 37.3395 - mse: 37.3395\n",
      "Epoch 284/500\n",
      "2/2 - 0s - loss: 33.7584 - mse: 33.7584\n",
      "Epoch 285/500\n",
      "2/2 - 0s - loss: 35.8700 - mse: 35.8700\n",
      "Epoch 286/500\n",
      "2/2 - 0s - loss: 32.5888 - mse: 32.5888\n",
      "Epoch 287/500\n",
      "2/2 - 0s - loss: 34.1290 - mse: 34.1290\n",
      "Epoch 288/500\n",
      "2/2 - 0s - loss: 37.7113 - mse: 37.7113\n",
      "Epoch 289/500\n",
      "2/2 - 0s - loss: 34.1153 - mse: 34.1153\n",
      "Epoch 290/500\n",
      "2/2 - 0s - loss: 31.5805 - mse: 31.5805\n",
      "Epoch 291/500\n",
      "2/2 - 0s - loss: 32.9417 - mse: 32.9417\n",
      "Epoch 292/500\n",
      "2/2 - 0s - loss: 31.9458 - mse: 31.9458\n",
      "Epoch 293/500\n",
      "2/2 - 0s - loss: 31.9516 - mse: 31.9516\n",
      "Epoch 294/500\n",
      "2/2 - 0s - loss: 32.1187 - mse: 32.1187\n",
      "Epoch 295/500\n",
      "2/2 - 0s - loss: 33.2943 - mse: 33.2943\n",
      "Epoch 296/500\n",
      "2/2 - 0s - loss: 30.5878 - mse: 30.5878\n",
      "Epoch 297/500\n",
      "2/2 - 0s - loss: 37.1887 - mse: 37.1887\n",
      "Epoch 298/500\n",
      "2/2 - 0s - loss: 32.5910 - mse: 32.5910\n",
      "Epoch 299/500\n",
      "2/2 - 0s - loss: 33.6904 - mse: 33.6904\n",
      "Epoch 300/500\n",
      "2/2 - 0s - loss: 31.9441 - mse: 31.9441\n",
      "Epoch 301/500\n",
      "2/2 - 0s - loss: 29.7078 - mse: 29.7078\n",
      "Epoch 302/500\n",
      "2/2 - 0s - loss: 29.3324 - mse: 29.3324\n",
      "Epoch 303/500\n",
      "2/2 - 0s - loss: 29.8970 - mse: 29.8970\n",
      "Epoch 304/500\n",
      "2/2 - 0s - loss: 30.3105 - mse: 30.3105\n",
      "Epoch 305/500\n",
      "2/2 - 0s - loss: 28.3591 - mse: 28.3591\n",
      "Epoch 306/500\n",
      "2/2 - 0s - loss: 28.3214 - mse: 28.3214\n",
      "Epoch 307/500\n",
      "2/2 - 0s - loss: 28.6467 - mse: 28.6467\n",
      "Epoch 308/500\n",
      "2/2 - 0s - loss: 37.2491 - mse: 37.2491\n",
      "Epoch 309/500\n",
      "2/2 - 0s - loss: 33.1972 - mse: 33.1972\n",
      "Epoch 310/500\n",
      "2/2 - 0s - loss: 27.6991 - mse: 27.6991\n",
      "Epoch 311/500\n",
      "2/2 - 0s - loss: 27.2430 - mse: 27.2430\n",
      "Epoch 312/500\n",
      "2/2 - 0s - loss: 27.4101 - mse: 27.4101\n",
      "Epoch 313/500\n",
      "2/2 - 0s - loss: 27.1764 - mse: 27.1764\n",
      "Epoch 314/500\n",
      "2/2 - 0s - loss: 27.2052 - mse: 27.2052\n",
      "Epoch 315/500\n",
      "2/2 - 0s - loss: 29.7047 - mse: 29.7047\n",
      "Epoch 316/500\n",
      "2/2 - 0s - loss: 30.7532 - mse: 30.7532\n",
      "Epoch 317/500\n",
      "2/2 - 0s - loss: 26.1423 - mse: 26.1423\n",
      "Epoch 318/500\n",
      "2/2 - 0s - loss: 27.9533 - mse: 27.9533\n",
      "Epoch 319/500\n",
      "2/2 - 0s - loss: 27.3304 - mse: 27.3304\n",
      "Epoch 320/500\n",
      "2/2 - 0s - loss: 26.8943 - mse: 26.8943\n",
      "Epoch 321/500\n",
      "2/2 - 0s - loss: 27.5846 - mse: 27.5846\n",
      "Epoch 322/500\n",
      "2/2 - 0s - loss: 27.5454 - mse: 27.5454\n",
      "Epoch 323/500\n",
      "2/2 - 0s - loss: 25.6086 - mse: 25.6086\n",
      "Epoch 324/500\n",
      "2/2 - 0s - loss: 25.0792 - mse: 25.0792\n",
      "Epoch 325/500\n",
      "2/2 - 0s - loss: 29.3856 - mse: 29.3856\n",
      "Epoch 326/500\n",
      "2/2 - 0s - loss: 31.0146 - mse: 31.0146\n",
      "Epoch 327/500\n",
      "2/2 - 0s - loss: 24.1495 - mse: 24.1495\n",
      "Epoch 328/500\n",
      "2/2 - 0s - loss: 24.9196 - mse: 24.9196\n",
      "Epoch 329/500\n",
      "2/2 - 0s - loss: 27.3635 - mse: 27.3635\n",
      "Epoch 330/500\n",
      "2/2 - 0s - loss: 26.7622 - mse: 26.7622\n",
      "Epoch 331/500\n",
      "2/2 - 0s - loss: 27.1506 - mse: 27.1506\n",
      "Epoch 332/500\n",
      "2/2 - 0s - loss: 25.3491 - mse: 25.3491\n",
      "Epoch 333/500\n",
      "2/2 - 0s - loss: 24.1582 - mse: 24.1582\n",
      "Epoch 334/500\n",
      "2/2 - 0s - loss: 25.9514 - mse: 25.9514\n",
      "Epoch 335/500\n",
      "2/2 - 0s - loss: 23.0847 - mse: 23.0847\n",
      "Epoch 336/500\n",
      "2/2 - 0s - loss: 23.5261 - mse: 23.5261\n",
      "Epoch 337/500\n",
      "2/2 - 0s - loss: 23.7103 - mse: 23.7103\n",
      "Epoch 338/500\n",
      "2/2 - 0s - loss: 27.8946 - mse: 27.8946\n",
      "Epoch 339/500\n",
      "2/2 - 0s - loss: 25.4298 - mse: 25.4298\n",
      "Epoch 340/500\n",
      "2/2 - 0s - loss: 22.2868 - mse: 22.2868\n",
      "Epoch 341/500\n",
      "2/2 - 0s - loss: 24.5791 - mse: 24.5791\n",
      "Epoch 342/500\n",
      "2/2 - 0s - loss: 24.5589 - mse: 24.5589\n",
      "Epoch 343/500\n",
      "2/2 - 0s - loss: 21.6872 - mse: 21.6872\n",
      "Epoch 344/500\n",
      "2/2 - 0s - loss: 22.5015 - mse: 22.5015\n",
      "Epoch 345/500\n",
      "2/2 - 0s - loss: 25.1354 - mse: 25.1354\n",
      "Epoch 346/500\n",
      "2/2 - 0s - loss: 22.7924 - mse: 22.7924\n",
      "Epoch 347/500\n",
      "2/2 - 0s - loss: 21.2814 - mse: 21.2814\n",
      "Epoch 348/500\n",
      "2/2 - 0s - loss: 21.1312 - mse: 21.1312\n",
      "Epoch 349/500\n",
      "2/2 - 0s - loss: 21.5178 - mse: 21.5178\n",
      "Epoch 350/500\n",
      "2/2 - 0s - loss: 24.8075 - mse: 24.8075\n",
      "Epoch 351/500\n",
      "2/2 - 0s - loss: 20.7535 - mse: 20.7535\n",
      "Epoch 352/500\n",
      "2/2 - 0s - loss: 20.6497 - mse: 20.6497\n",
      "Epoch 353/500\n",
      "2/2 - 0s - loss: 20.4374 - mse: 20.4374\n",
      "Epoch 354/500\n",
      "2/2 - 0s - loss: 20.4308 - mse: 20.4308\n",
      "Epoch 355/500\n",
      "2/2 - 0s - loss: 23.7008 - mse: 23.7008\n",
      "Epoch 356/500\n",
      "2/2 - 0s - loss: 20.0767 - mse: 20.0767\n",
      "Epoch 357/500\n",
      "2/2 - 0s - loss: 22.3906 - mse: 22.3906\n",
      "Epoch 358/500\n",
      "2/2 - 0s - loss: 20.0707 - mse: 20.0707\n",
      "Epoch 359/500\n",
      "2/2 - 0s - loss: 22.8723 - mse: 22.8723\n",
      "Epoch 360/500\n",
      "2/2 - 0s - loss: 20.0480 - mse: 20.0480\n",
      "Epoch 361/500\n",
      "2/2 - 0s - loss: 21.9247 - mse: 21.9247\n",
      "Epoch 362/500\n",
      "2/2 - 0s - loss: 19.0856 - mse: 19.0856\n",
      "Epoch 363/500\n",
      "2/2 - 0s - loss: 19.5586 - mse: 19.5586\n",
      "Epoch 364/500\n",
      "2/2 - 0s - loss: 19.0440 - mse: 19.0440\n",
      "Epoch 365/500\n",
      "2/2 - 0s - loss: 19.5376 - mse: 19.5376\n",
      "Epoch 366/500\n",
      "2/2 - 0s - loss: 19.1875 - mse: 19.1875\n",
      "Epoch 367/500\n",
      "2/2 - 0s - loss: 20.7638 - mse: 20.7638\n",
      "Epoch 368/500\n",
      "2/2 - 0s - loss: 19.8312 - mse: 19.8312\n",
      "Epoch 369/500\n",
      "2/2 - 0s - loss: 18.3040 - mse: 18.3040\n",
      "Epoch 370/500\n",
      "2/2 - 0s - loss: 19.4262 - mse: 19.4262\n",
      "Epoch 371/500\n",
      "2/2 - 0s - loss: 22.0684 - mse: 22.0684\n",
      "Epoch 372/500\n",
      "2/2 - 0s - loss: 18.3953 - mse: 18.3953\n",
      "Epoch 373/500\n",
      "2/2 - 0s - loss: 20.1359 - mse: 20.1359\n",
      "Epoch 374/500\n",
      "2/2 - 0s - loss: 22.5872 - mse: 22.5872\n",
      "Epoch 375/500\n",
      "2/2 - 0s - loss: 19.9818 - mse: 19.9818\n",
      "Epoch 376/500\n",
      "2/2 - 0s - loss: 17.7693 - mse: 17.7693\n",
      "Epoch 377/500\n",
      "2/2 - 0s - loss: 18.1603 - mse: 18.1603\n",
      "Epoch 378/500\n",
      "2/2 - 0s - loss: 18.5182 - mse: 18.5182\n",
      "Epoch 379/500\n",
      "2/2 - 0s - loss: 17.4035 - mse: 17.4035\n",
      "Epoch 380/500\n",
      "2/2 - 0s - loss: 17.6660 - mse: 17.6660\n",
      "Epoch 381/500\n",
      "2/2 - 0s - loss: 17.0607 - mse: 17.0607\n",
      "Epoch 382/500\n",
      "2/2 - 0s - loss: 16.3611 - mse: 16.3611\n",
      "Epoch 383/500\n",
      "2/2 - 0s - loss: 17.6025 - mse: 17.6025\n",
      "Epoch 384/500\n",
      "2/2 - 0s - loss: 16.6681 - mse: 16.6681\n",
      "Epoch 385/500\n",
      "2/2 - 0s - loss: 16.7385 - mse: 16.7385\n",
      "Epoch 386/500\n",
      "2/2 - 0s - loss: 23.1070 - mse: 23.1070\n",
      "Epoch 387/500\n",
      "2/2 - 0s - loss: 16.3433 - mse: 16.3433\n",
      "Epoch 388/500\n",
      "2/2 - 0s - loss: 16.3882 - mse: 16.3882\n",
      "Epoch 389/500\n",
      "2/2 - 0s - loss: 16.1359 - mse: 16.1359\n",
      "Epoch 390/500\n",
      "2/2 - 0s - loss: 16.4161 - mse: 16.4161\n",
      "Epoch 391/500\n",
      "2/2 - 0s - loss: 24.0120 - mse: 24.0120\n",
      "Epoch 392/500\n",
      "2/2 - 0s - loss: 15.5617 - mse: 15.5617\n",
      "Epoch 393/500\n",
      "2/2 - 0s - loss: 15.1688 - mse: 15.1688\n",
      "Epoch 394/500\n",
      "2/2 - 0s - loss: 14.9881 - mse: 14.9881\n",
      "Epoch 395/500\n",
      "2/2 - 0s - loss: 15.6484 - mse: 15.6484\n",
      "Epoch 396/500\n",
      "2/2 - 0s - loss: 14.8039 - mse: 14.8039\n",
      "Epoch 397/500\n",
      "2/2 - 0s - loss: 14.6511 - mse: 14.6511\n",
      "Epoch 398/500\n",
      "2/2 - 0s - loss: 18.4504 - mse: 18.4504\n",
      "Epoch 399/500\n",
      "2/2 - 0s - loss: 16.1307 - mse: 16.1307\n",
      "Epoch 400/500\n",
      "2/2 - 0s - loss: 15.4410 - mse: 15.4410\n",
      "Epoch 401/500\n",
      "2/2 - 0s - loss: 16.6050 - mse: 16.6050\n",
      "Epoch 402/500\n",
      "2/2 - 0s - loss: 14.8244 - mse: 14.8244\n",
      "Epoch 403/500\n",
      "2/2 - 0s - loss: 15.9475 - mse: 15.9475\n",
      "Epoch 404/500\n",
      "2/2 - 0s - loss: 14.9708 - mse: 14.9708\n",
      "Epoch 405/500\n",
      "2/2 - 0s - loss: 14.2273 - mse: 14.2273\n",
      "Epoch 406/500\n",
      "2/2 - 0s - loss: 20.5701 - mse: 20.5701\n",
      "Epoch 407/500\n",
      "2/2 - 0s - loss: 17.3166 - mse: 17.3166\n",
      "Epoch 408/500\n",
      "2/2 - 0s - loss: 13.5844 - mse: 13.5844\n",
      "Epoch 409/500\n",
      "2/2 - 0s - loss: 13.5695 - mse: 13.5695\n",
      "Epoch 410/500\n",
      "2/2 - 0s - loss: 14.4812 - mse: 14.4812\n",
      "Epoch 411/500\n",
      "2/2 - 0s - loss: 14.2128 - mse: 14.2128\n",
      "Epoch 412/500\n",
      "2/2 - 0s - loss: 13.7950 - mse: 13.7950\n",
      "Epoch 413/500\n",
      "2/2 - 0s - loss: 13.9566 - mse: 13.9566\n",
      "Epoch 414/500\n",
      "2/2 - 0s - loss: 17.3660 - mse: 17.3660\n",
      "Epoch 415/500\n",
      "2/2 - 0s - loss: 13.2675 - mse: 13.2675\n",
      "Epoch 416/500\n",
      "2/2 - 0s - loss: 14.6294 - mse: 14.6294\n",
      "Epoch 417/500\n",
      "2/2 - 0s - loss: 12.8213 - mse: 12.8213\n",
      "Epoch 418/500\n",
      "2/2 - 0s - loss: 14.4646 - mse: 14.4646\n",
      "Epoch 419/500\n",
      "2/2 - 0s - loss: 14.0451 - mse: 14.0451\n",
      "Epoch 420/500\n",
      "2/2 - 0s - loss: 14.5044 - mse: 14.5044\n",
      "Epoch 421/500\n",
      "2/2 - 0s - loss: 12.9339 - mse: 12.9339\n",
      "Epoch 422/500\n",
      "2/2 - 0s - loss: 12.2727 - mse: 12.2727\n",
      "Epoch 423/500\n",
      "2/2 - 0s - loss: 13.9968 - mse: 13.9968\n",
      "Epoch 424/500\n",
      "2/2 - 0s - loss: 14.5478 - mse: 14.5478\n",
      "Epoch 425/500\n",
      "2/2 - 0s - loss: 13.0324 - mse: 13.0324\n",
      "Epoch 426/500\n",
      "2/2 - 0s - loss: 14.2773 - mse: 14.2773\n",
      "Epoch 427/500\n",
      "2/2 - 0s - loss: 17.4969 - mse: 17.4969\n",
      "Epoch 428/500\n",
      "2/2 - 0s - loss: 11.9127 - mse: 11.9127\n",
      "Epoch 429/500\n",
      "2/2 - 0s - loss: 13.4780 - mse: 13.4780\n",
      "Epoch 430/500\n",
      "2/2 - 0s - loss: 12.2123 - mse: 12.2123\n",
      "Epoch 431/500\n",
      "2/2 - 0s - loss: 12.5430 - mse: 12.5430\n",
      "Epoch 432/500\n",
      "2/2 - 0s - loss: 12.5143 - mse: 12.5143\n",
      "Epoch 433/500\n",
      "2/2 - 0s - loss: 11.8069 - mse: 11.8069\n",
      "Epoch 434/500\n",
      "2/2 - 0s - loss: 16.5706 - mse: 16.5706\n",
      "Epoch 435/500\n",
      "2/2 - 0s - loss: 13.0737 - mse: 13.0737\n",
      "Epoch 436/500\n",
      "2/2 - 0s - loss: 12.6457 - mse: 12.6457\n",
      "Epoch 437/500\n",
      "2/2 - 0s - loss: 14.0765 - mse: 14.0765\n",
      "Epoch 438/500\n",
      "2/2 - 0s - loss: 11.3415 - mse: 11.3415\n",
      "Epoch 439/500\n",
      "2/2 - 0s - loss: 10.9841 - mse: 10.9841\n",
      "Epoch 440/500\n",
      "2/2 - 0s - loss: 10.9056 - mse: 10.9056\n",
      "Epoch 441/500\n",
      "2/2 - 0s - loss: 11.2779 - mse: 11.2779\n",
      "Epoch 442/500\n",
      "2/2 - 0s - loss: 11.9312 - mse: 11.9312\n",
      "Epoch 443/500\n",
      "2/2 - 0s - loss: 12.8558 - mse: 12.8558\n",
      "Epoch 444/500\n",
      "2/2 - 0s - loss: 11.1223 - mse: 11.1223\n",
      "Epoch 445/500\n",
      "2/2 - 0s - loss: 11.3915 - mse: 11.3915\n",
      "Epoch 446/500\n",
      "2/2 - 0s - loss: 14.5138 - mse: 14.5138\n",
      "Epoch 447/500\n",
      "2/2 - 0s - loss: 13.1889 - mse: 13.1889\n",
      "Epoch 448/500\n",
      "2/2 - 0s - loss: 11.5173 - mse: 11.5173\n",
      "Epoch 449/500\n",
      "2/2 - 0s - loss: 10.2888 - mse: 10.2888\n",
      "Epoch 450/500\n",
      "2/2 - 0s - loss: 10.8480 - mse: 10.8480\n",
      "Epoch 451/500\n",
      "2/2 - 0s - loss: 10.5656 - mse: 10.5656\n",
      "Epoch 452/500\n",
      "2/2 - 0s - loss: 10.2315 - mse: 10.2315\n",
      "Epoch 453/500\n",
      "2/2 - 0s - loss: 10.0966 - mse: 10.0966\n",
      "Epoch 454/500\n",
      "2/2 - 0s - loss: 10.0994 - mse: 10.0994\n",
      "Epoch 455/500\n",
      "2/2 - 0s - loss: 13.3825 - mse: 13.3825\n",
      "Epoch 456/500\n",
      "2/2 - 0s - loss: 13.3518 - mse: 13.3518\n",
      "Epoch 457/500\n",
      "2/2 - 0s - loss: 11.4057 - mse: 11.4057\n",
      "Epoch 458/500\n",
      "2/2 - 0s - loss: 10.9179 - mse: 10.9179\n",
      "Epoch 459/500\n",
      "2/2 - 0s - loss: 11.7860 - mse: 11.7860\n",
      "Epoch 460/500\n",
      "2/2 - 0s - loss: 9.4406 - mse: 9.4406\n",
      "Epoch 461/500\n",
      "2/2 - 0s - loss: 9.4746 - mse: 9.4746\n",
      "Epoch 462/500\n",
      "2/2 - 0s - loss: 11.9875 - mse: 11.9875\n",
      "Epoch 463/500\n",
      "2/2 - 0s - loss: 11.0093 - mse: 11.0093\n",
      "Epoch 464/500\n",
      "2/2 - 0s - loss: 11.3669 - mse: 11.3669\n",
      "Epoch 465/500\n",
      "2/2 - 0s - loss: 9.7696 - mse: 9.7696\n",
      "Epoch 466/500\n",
      "2/2 - 0s - loss: 10.4047 - mse: 10.4047\n",
      "Epoch 467/500\n",
      "2/2 - 0s - loss: 9.7499 - mse: 9.7499\n",
      "Epoch 468/500\n",
      "2/2 - 0s - loss: 9.4053 - mse: 9.4053\n",
      "Epoch 469/500\n",
      "2/2 - 0s - loss: 12.6378 - mse: 12.6378\n",
      "Epoch 470/500\n",
      "2/2 - 0s - loss: 9.8687 - mse: 9.8687\n",
      "Epoch 471/500\n",
      "2/2 - 0s - loss: 8.7654 - mse: 8.7654\n",
      "Epoch 472/500\n",
      "2/2 - 0s - loss: 9.1256 - mse: 9.1256\n",
      "Epoch 473/500\n",
      "2/2 - 0s - loss: 11.6129 - mse: 11.6129\n",
      "Epoch 474/500\n",
      "2/2 - 0s - loss: 9.2155 - mse: 9.2155\n",
      "Epoch 475/500\n",
      "2/2 - 0s - loss: 8.5017 - mse: 8.5017\n",
      "Epoch 476/500\n",
      "2/2 - 0s - loss: 8.5866 - mse: 8.5866\n",
      "Epoch 477/500\n",
      "2/2 - 0s - loss: 8.6216 - mse: 8.6216\n",
      "Epoch 478/500\n",
      "2/2 - 0s - loss: 9.7145 - mse: 9.7145\n",
      "Epoch 479/500\n",
      "2/2 - 0s - loss: 9.3649 - mse: 9.3649\n",
      "Epoch 480/500\n",
      "2/2 - 0s - loss: 9.1937 - mse: 9.1937\n",
      "Epoch 481/500\n",
      "2/2 - 0s - loss: 9.1098 - mse: 9.1098\n",
      "Epoch 482/500\n",
      "2/2 - 0s - loss: 9.9253 - mse: 9.9253\n",
      "Epoch 483/500\n",
      "2/2 - 0s - loss: 11.7016 - mse: 11.7016\n",
      "Epoch 484/500\n",
      "2/2 - 0s - loss: 7.9679 - mse: 7.9679\n",
      "Epoch 485/500\n",
      "2/2 - 0s - loss: 7.9957 - mse: 7.9957\n",
      "Epoch 486/500\n",
      "2/2 - 0s - loss: 10.7030 - mse: 10.7030\n",
      "Epoch 487/500\n",
      "2/2 - 0s - loss: 10.7418 - mse: 10.7418\n",
      "Epoch 488/500\n",
      "2/2 - 0s - loss: 9.4850 - mse: 9.4850\n",
      "Epoch 489/500\n",
      "2/2 - 0s - loss: 9.1280 - mse: 9.1280\n",
      "Epoch 490/500\n",
      "2/2 - 0s - loss: 12.3614 - mse: 12.3614\n",
      "Epoch 491/500\n",
      "2/2 - 0s - loss: 9.3268 - mse: 9.3268\n",
      "Epoch 492/500\n",
      "2/2 - 0s - loss: 8.1353 - mse: 8.1353\n",
      "Epoch 493/500\n",
      "2/2 - 0s - loss: 8.7830 - mse: 8.7830\n",
      "Epoch 494/500\n",
      "2/2 - 0s - loss: 7.7302 - mse: 7.7302\n",
      "Epoch 495/500\n",
      "2/2 - 0s - loss: 7.5269 - mse: 7.5269\n",
      "Epoch 496/500\n",
      "2/2 - 0s - loss: 7.7221 - mse: 7.7221\n",
      "Epoch 497/500\n",
      "2/2 - 0s - loss: 7.5026 - mse: 7.5026\n",
      "Epoch 498/500\n",
      "2/2 - 0s - loss: 7.2692 - mse: 7.2692\n",
      "Epoch 499/500\n",
      "2/2 - 0s - loss: 8.0030 - mse: 8.0030\n",
      "Epoch 500/500\n",
      "2/2 - 0s - loss: 7.0911 - mse: 7.0911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2807924f0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', metrics=['mse'])\n",
    "\n",
    "model.fit(X, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:13:26.631300: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>22.246986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.401917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>18.858179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.135105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.835787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.834690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>18.440342</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.978014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>16.650234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.236450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid  pred_linear\n",
       "abbrev                                               \n",
       "AL       18.8    22.246986           1.0    28.401917\n",
       "AK       18.1    18.858179           1.0    10.135105\n",
       "AZ       18.6    16.835787           1.0     8.834690\n",
       "AR       22.4    18.440342           1.0    25.978014\n",
       "CA       12.0    16.650234           1.0    28.236450"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "dfsel['pred_linear'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tanh` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:19:12.519538: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282ecb310>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='tanh'))\n",
    "\n",
    "model.compile(loss='mse', metrics=['mse'])\n",
    "\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:19:16.656994: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>22.246986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.401917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>18.858179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.135105</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.835787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.834690</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>18.440342</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.978014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>16.650234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.236450</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid  pred_linear  pred_tanh\n",
       "abbrev                                                          \n",
       "AL       18.8    22.246986           1.0    28.401917        1.0\n",
       "AK       18.1    18.858179           1.0    10.135105        1.0\n",
       "AZ       18.6    16.835787           1.0     8.834690        1.0\n",
       "AR       22.4    18.440342           1.0    25.978014        1.0\n",
       "CA       12.0    16.650234           1.0    28.236450        1.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "dfsel['pred_tanh'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `relu` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:24:32.946418: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2854ed400>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(layer=Dense(units=1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse', metrics=['mse'])\n",
    "\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:24:37.765121: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>22.246986</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.401917</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>18.858179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.135105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>16.835787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.834690</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>18.440342</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.978014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>16.650234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.236450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid  pred_linear  pred_tanh  pred_relu\n",
       "abbrev                                                                     \n",
       "AL       18.8    22.246986           1.0    28.401917        1.0        0.0\n",
       "AK       18.1    18.858179           1.0    10.135105        1.0        0.0\n",
       "AZ       18.6    16.835787           1.0     8.834690        1.0        0.0\n",
       "AR       22.4    18.440342           1.0    25.978014        1.0        0.0\n",
       "CA       12.0    16.650234           1.0    28.236450        1.0        0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "dfsel['pred_relu'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the predictions changing? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16023340370>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', metrics=['mse'])\n",
    "\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>20.272116</td>\n",
       "      <td>20.272116</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>20.923376</td>\n",
       "      <td>20.923376</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>20.144960</td>\n",
       "      <td>20.144960</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>22.679586</td>\n",
       "      <td>22.679586</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>13.580787</td>\n",
       "      <td>13.580787</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid  pred_linear  pred_tanh  pred_relu\n",
       "abbrev                                                                     \n",
       "AL       18.8    20.272116     20.272116     0.973202       -1.0        0.0\n",
       "AK       18.1    20.923376     20.923376     0.973202       -1.0        0.0\n",
       "AZ       18.6    20.144960     20.144960     0.973202       -1.0        0.0\n",
       "AR       22.4    22.679586     22.679586     0.973202       -1.0        0.0\n",
       "CA       12.0    13.580787     13.580787     0.973202       -1.0        0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "dfsel['pred_linear'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.691730806098838"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/optimizers/#available-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers comparison in GIF ‚Üí https://mlfromscratch.com/optimizers-explained/#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesla's Neural Network Models is composed of 48 models trainned in 70.000 hours of GPU ‚Üí https://tesla.com/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Year with a 8 GPU Computer ‚Üí https://twitter.com/thirdrowtesla/status/1252723358342377472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:03:37.036943: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfsel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_43422/902708605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfsel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_sgd'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfsel' is not defined"
     ]
    }
   ],
   "source": [
    "dfsel['pred_sgd'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pred_sgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10668/4078616456.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfsel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdfsel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_sgd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pred_sgd'"
     ]
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sgd)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.76659054,  0.01941717,  0.6998006 ],\n",
       "        [ 0.27831292, -0.7375144 , -0.23064262],\n",
       "        [-0.18085045,  0.6981566 , -0.4463861 ],\n",
       "        [ 0.54964554,  0.27596498, -0.65408474],\n",
       "        [-0.45607024,  0.72699857, -0.5107513 ],\n",
       "        [ 0.3142885 ,  0.53404367, -0.56087637]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 0.30465853],\n",
       "        [-0.1850338 ],\n",
       "        [-0.07273304]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10668/3041766850.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `ADAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "history = model.fit(X, y, epochs=500, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "      <th>pred_gsd</th>\n",
       "      <th>pred_adam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>20.272116</td>\n",
       "      <td>20.272116</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.502316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>20.923376</td>\n",
       "      <td>20.923376</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.745525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>20.144960</td>\n",
       "      <td>20.144960</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.255413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>22.679586</td>\n",
       "      <td>22.679586</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.887188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>13.580787</td>\n",
       "      <td>13.580787</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.768291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid  pred_linear  pred_tanh  pred_relu  \\\n",
       "abbrev                                                                        \n",
       "AL       18.8    20.272116     20.272116     0.973202       -1.0        0.0   \n",
       "AK       18.1    20.923376     20.923376     0.973202       -1.0        0.0   \n",
       "AZ       18.6    20.144960     20.144960     0.973202       -1.0        0.0   \n",
       "AR       22.4    22.679586     22.679586     0.973202       -1.0        0.0   \n",
       "CA       12.0    13.580787     13.580787     0.973202       -1.0        0.0   \n",
       "\n",
       "        pred_gsd  pred_adam  \n",
       "abbrev                       \n",
       "AL           0.0  19.502316  \n",
       "AK           0.0  17.745525  \n",
       "AZ           0.0  17.255413  \n",
       "AR           0.0  19.887188  \n",
       "CA           0.0  13.768291  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "dfsel['pred_adam'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5727062906881657"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_adam)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `RMSPROP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])\n",
    "\n",
    "history = model.fit(X, y, epochs=500, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "      <th>pred_gsd</th>\n",
       "      <th>pred_adam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>20.272116</td>\n",
       "      <td>20.272116</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.095829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>20.923376</td>\n",
       "      <td>20.923376</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.338139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>20.144960</td>\n",
       "      <td>20.144960</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.384342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>22.679586</td>\n",
       "      <td>22.679586</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.949863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>13.580787</td>\n",
       "      <td>13.580787</td>\n",
       "      <td>0.973202</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.331272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_sigmoid  pred_linear  pred_tanh  pred_relu  \\\n",
       "abbrev                                                                        \n",
       "AL       18.8    20.272116     20.272116     0.973202       -1.0        0.0   \n",
       "AK       18.1    20.923376     20.923376     0.973202       -1.0        0.0   \n",
       "AZ       18.6    20.144960     20.144960     0.973202       -1.0        0.0   \n",
       "AR       22.4    22.679586     22.679586     0.973202       -1.0        0.0   \n",
       "CA       12.0    13.580787     13.580787     0.973202       -1.0        0.0   \n",
       "\n",
       "        pred_gsd  pred_adam  \n",
       "abbrev                       \n",
       "AL           0.0  16.095829  \n",
       "AK           0.0  11.338139  \n",
       "AZ           0.0  10.384342  \n",
       "AR           0.0  15.949863  \n",
       "CA           0.0  12.331272  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "dfsel['pred_adam'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.014740867368957"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_adam)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it take different times to get the best accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:28:37.559776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 2/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 3/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 4/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 5/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 6/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 7/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:28:37.785300: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 9/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 10/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 11/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 12/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 13/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 14/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 15/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 16/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 17/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 18/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 19/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 20/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 21/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 22/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 23/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 24/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 25/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 26/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 27/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 28/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 29/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 30/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 31/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 32/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 33/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 34/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 35/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 36/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 37/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 38/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 39/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 40/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 41/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 42/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 43/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 44/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 45/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 46/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 47/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 48/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 49/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 50/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 51/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 52/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 53/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 54/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 55/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 56/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 57/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 58/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 59/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 60/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 61/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 62/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 63/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 64/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 65/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 66/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 67/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 68/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 69/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 70/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 71/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 72/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 73/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 74/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 75/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 76/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 77/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 78/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 79/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 80/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 81/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 82/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 83/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 84/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 85/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 86/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 87/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 88/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 89/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 90/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 91/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 92/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 93/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 94/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 95/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 96/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 97/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 98/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 99/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 100/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 101/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 102/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 103/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 104/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 105/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 106/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 107/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 108/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 109/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 110/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 111/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 112/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 113/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 114/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 115/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 116/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 117/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 118/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 119/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 120/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 121/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 122/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 123/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 124/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 125/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 126/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 127/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 128/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 129/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 130/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 131/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 132/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 133/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 134/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 135/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 136/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 137/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 138/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 139/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 140/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 141/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 142/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 143/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 144/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 145/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 146/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 147/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 148/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 149/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 150/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 151/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 152/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 153/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 154/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 155/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 156/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 157/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 158/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 159/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 160/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 161/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 162/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 163/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 164/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 165/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 166/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 167/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 168/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 169/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 170/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 171/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 172/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 173/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 174/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 175/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 176/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 177/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 178/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 179/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 180/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 181/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 182/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 183/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 184/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 185/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 186/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 187/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 188/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 189/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 190/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 191/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 192/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 193/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 194/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 195/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 196/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 197/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 198/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 199/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 200/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 201/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 202/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 203/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 204/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 205/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 206/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 207/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 208/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 209/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 210/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 211/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 212/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 213/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 214/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 215/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 216/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 217/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 218/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 219/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 220/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 221/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 222/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 223/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 224/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 225/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 226/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 227/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 228/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 229/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 230/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 231/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 232/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 233/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 234/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 235/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 236/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 237/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 238/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 239/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 240/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 241/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 242/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 243/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 244/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 245/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 246/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 247/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 248/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 249/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 250/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 251/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 252/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 253/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 254/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 255/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 256/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 257/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 258/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 259/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 260/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 261/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 262/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 263/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 264/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 265/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 266/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 267/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 268/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 269/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 270/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 271/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 272/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 273/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 274/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 275/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 276/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 277/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 278/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 279/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 280/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 281/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 282/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 283/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 284/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 285/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 286/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 287/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 288/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 289/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 290/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 291/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 292/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 293/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 294/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 295/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 296/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 297/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 298/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 299/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 300/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 301/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 302/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 303/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 304/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 305/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 306/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 307/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 308/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 309/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 310/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 311/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 312/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 313/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 314/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 315/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 316/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 317/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 318/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 319/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 320/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 321/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 322/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 323/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 324/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 325/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 326/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 327/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 328/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 329/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 330/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 331/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 332/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 333/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 334/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 335/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 336/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 337/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 338/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 339/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 340/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 341/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 342/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 343/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 344/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 345/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 346/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 347/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 348/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 349/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 350/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 351/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 352/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 353/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 354/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 355/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 356/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 357/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 358/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 359/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 360/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 361/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 362/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 363/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 364/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 365/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 366/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 367/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 368/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 369/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 370/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 371/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 372/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 373/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 374/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 375/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 376/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 377/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 378/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 379/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 380/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 381/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 382/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 383/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 384/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 385/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 386/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 387/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 388/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 389/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 390/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 391/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 392/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 393/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 394/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 395/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 396/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 397/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 398/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 399/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 400/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 401/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 402/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 403/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 404/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 405/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 406/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 407/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 408/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 409/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 410/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 411/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 412/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 413/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 414/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 415/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 416/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 417/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 418/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 419/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 420/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 421/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 422/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 423/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 424/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 425/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 426/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 427/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 428/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 429/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 430/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 431/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 432/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 433/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 434/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 435/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 436/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 437/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 438/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 439/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 440/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 441/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 442/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 443/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 444/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 445/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 446/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 447/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 448/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 449/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 450/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 451/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 452/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 453/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 454/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 455/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 456/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 457/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 458/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 459/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 460/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 461/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 462/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 463/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 464/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 465/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 466/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 467/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 468/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 469/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 470/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 471/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 472/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 473/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 474/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 475/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 476/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 477/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 478/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 479/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 480/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 481/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 482/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 483/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 484/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 485/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 486/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 487/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 488/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 489/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 490/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 491/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 492/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 493/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 494/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 495/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 496/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9854 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 497/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 498/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 499/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9849 - val_loss: 252.1015 - val_mse: 4815.4844\n",
      "Epoch 500/500\n",
      "2/2 - 0s - loss: 239.6596 - mse: 4290.9858 - val_loss: 252.1015 - val_mse: 4815.4844\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['mse'])\n",
    "\n",
    "history = model.fit(X, y, epochs=500, verbose=2, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, what should be a feasible configuration of the Neural Network for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `kernel_initializer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `activation` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `optimizer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `loss` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's importance to find **Non-Linear Patterns** in the Data\n",
    "\n",
    "> - The number of Neurons & Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.87287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Mathematical Formula\n",
    "- Weights / Kernel Initializer\n",
    "- Loss Function\n",
    "- Activation Function\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What cannot you change arbitrarily of a Neural Network?\n",
    "\n",
    "- Input Neurons\n",
    "- Output Neurons\n",
    "- Loss Functions\n",
    "- Activation Functions"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jes√∫s L√≥pez @sotastica"
   }
  ],
  "interpreter": {
   "hash": "a2b8701b642343483c5f5e717bcb0768c6b951acf38d76a6fc8ea01492bda71d"
  },
  "kernelspec": {
   "display_name": "DeepLearning Python",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
